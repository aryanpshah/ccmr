
############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-11-29 20:13:23.023527: Using torch.compile...
2025-11-29 20:13:28.533998: do_dummy_2d_data_aug: False
2025-11-29 20:13:28.539724: Using splits from existing split file: /workspace/onedrive/ccmr/ccmr/data/nnunet/nnUNet_preprocessed/Dataset920_HVSMR_L20/splits_final.json
2025-11-29 20:13:28.545997: The split file contains 5 splits.
2025-11-29 20:13:28.548559: Desired fold for training: 0
2025-11-29 20:13:28.551220: This split has 16 training and 4 validation cases.
using pin_memory on device 0
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 160, 112], 'median_image_size_in_voxels': [127.5, 149.0, 110.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset920_HVSMR_L20', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [128, 149, 110], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.8231292963027954, 'mean': 0.7315230369567871, 'median': 0.7581771612167358, 'min': -0.03358686342835426, 'percentile_00_5': 0.23230689764022827, 'percentile_99_5': 1.113864541053772, 'std': 0.16795307397842407}}} 

2025-11-29 20:13:41.080133: Unable to plot network architecture: nnUNet_compile is enabled!
2025-11-29 20:13:41.285272: 
2025-11-29 20:13:41.289120: Epoch 0
2025-11-29 20:13:41.293362: Current learning rate: 0.01
2025-11-29 20:17:25.716740: train_loss -0.1402
2025-11-29 20:17:25.723770: val_loss -0.3309
2025-11-29 20:17:25.728545: Pseudo dice [np.float32(0.0), np.float32(0.7477)]
2025-11-29 20:17:25.732750: Epoch time: 224.44 s
2025-11-29 20:17:25.738000: Yayy! New best EMA pseudo Dice: 0.37380000948905945
2025-11-29 20:17:27.692489: 
2025-11-29 20:17:27.698040: Epoch 1
2025-11-29 20:17:27.711645: Current learning rate: 0.00999
2025-11-29 20:20:46.961883: train_loss -0.3667
2025-11-29 20:20:46.968241: val_loss -0.3585
2025-11-29 20:20:46.970994: Pseudo dice [np.float32(0.6353), np.float32(0.7727)]
2025-11-29 20:20:46.973729: Epoch time: 199.27 s
2025-11-29 20:20:46.976092: Yayy! New best EMA pseudo Dice: 0.40689998865127563
2025-11-29 20:20:49.139467: 
2025-11-29 20:20:49.145715: Epoch 2
2025-11-29 20:20:49.152178: Current learning rate: 0.00998
2025-11-29 20:24:08.150718: train_loss -0.4919
2025-11-29 20:24:08.157315: val_loss -0.5075
2025-11-29 20:24:08.161167: Pseudo dice [np.float32(0.7194), np.float32(0.8075)]
2025-11-29 20:24:08.164401: Epoch time: 199.01 s
2025-11-29 20:24:08.168097: Yayy! New best EMA pseudo Dice: 0.4424999952316284
2025-11-29 20:24:10.033954: 
2025-11-29 20:24:10.037053: Epoch 3
2025-11-29 20:24:10.039973: Current learning rate: 0.00997
2025-11-29 20:27:29.079646: train_loss -0.5684
2025-11-29 20:27:29.089956: val_loss -0.5575
2025-11-29 20:27:29.093083: Pseudo dice [np.float32(0.7711), np.float32(0.8257)]
2025-11-29 20:27:29.095917: Epoch time: 199.05 s
2025-11-29 20:27:29.098482: Yayy! New best EMA pseudo Dice: 0.4781000018119812
2025-11-29 20:27:31.092815: 
2025-11-29 20:27:31.098750: Epoch 4
2025-11-29 20:27:31.112957: Current learning rate: 0.00996
2025-11-29 20:30:50.128500: train_loss -0.6303
2025-11-29 20:30:50.132677: val_loss -0.5342
2025-11-29 20:30:50.135826: Pseudo dice [np.float32(0.776), np.float32(0.851)]
2025-11-29 20:30:50.138484: Epoch time: 199.04 s
2025-11-29 20:30:50.141090: Yayy! New best EMA pseudo Dice: 0.5116000175476074
2025-11-29 20:30:52.024224: 
2025-11-29 20:30:52.027173: Epoch 5
2025-11-29 20:30:52.029604: Current learning rate: 0.00995
2025-11-29 20:34:10.969355: train_loss -0.6731
2025-11-29 20:34:10.978519: val_loss -0.563
2025-11-29 20:34:11.010929: Pseudo dice [np.float32(0.7828), np.float32(0.8497)]
2025-11-29 20:34:11.013870: Epoch time: 198.95 s
2025-11-29 20:34:11.019965: Yayy! New best EMA pseudo Dice: 0.5421000123023987
2025-11-29 20:34:13.018128: 
2025-11-29 20:34:13.021123: Epoch 6
2025-11-29 20:34:13.024112: Current learning rate: 0.00995
2025-11-29 20:37:32.038648: train_loss -0.6594
2025-11-29 20:37:32.044295: val_loss -0.6004
2025-11-29 20:37:32.047432: Pseudo dice [np.float32(0.8116), np.float32(0.8651)]
2025-11-29 20:37:32.049591: Epoch time: 199.02 s
2025-11-29 20:37:32.061817: Yayy! New best EMA pseudo Dice: 0.5716999769210815
2025-11-29 20:37:33.946729: 
2025-11-29 20:37:33.949665: Epoch 7
2025-11-29 20:37:33.956349: Current learning rate: 0.00994
2025-11-29 20:40:52.926875: train_loss -0.7104
2025-11-29 20:40:52.931565: val_loss -0.5765
2025-11-29 20:40:52.934108: Pseudo dice [np.float32(0.8002), np.float32(0.8667)]
2025-11-29 20:40:52.937186: Epoch time: 198.98 s
2025-11-29 20:40:52.939770: Yayy! New best EMA pseudo Dice: 0.5978999733924866
2025-11-29 20:40:55.099138: 
2025-11-29 20:40:55.143289: Epoch 8
2025-11-29 20:40:55.146896: Current learning rate: 0.00993
2025-11-29 20:44:14.126570: train_loss -0.7289
2025-11-29 20:44:14.131160: val_loss -0.5938
2025-11-29 20:44:14.133780: Pseudo dice [np.float32(0.7851), np.float32(0.8594)]
2025-11-29 20:44:14.136410: Epoch time: 199.03 s
2025-11-29 20:44:14.139095: Yayy! New best EMA pseudo Dice: 0.6202999949455261
2025-11-29 20:44:16.050906: 
2025-11-29 20:44:16.053598: Epoch 9
2025-11-29 20:44:16.056695: Current learning rate: 0.00992
2025-11-29 20:47:35.048714: train_loss -0.7303
2025-11-29 20:47:35.053360: val_loss -0.4412
2025-11-29 20:47:35.056146: Pseudo dice [np.float32(0.678), np.float32(0.8336)]
2025-11-29 20:47:35.059142: Epoch time: 199.0 s
2025-11-29 20:47:35.062208: Yayy! New best EMA pseudo Dice: 0.633899986743927
2025-11-29 20:47:37.314532: 
2025-11-29 20:47:37.323257: Epoch 10
2025-11-29 20:47:37.326092: Current learning rate: 0.00991
2025-11-29 20:50:56.272341: train_loss -0.7645
2025-11-29 20:50:56.282007: val_loss -0.5918
2025-11-29 20:50:56.287071: Pseudo dice [np.float32(0.7469), np.float32(0.8614)]
2025-11-29 20:50:56.298289: Epoch time: 198.96 s
2025-11-29 20:50:56.322301: Yayy! New best EMA pseudo Dice: 0.6509000062942505
2025-11-29 20:50:58.410527: 
2025-11-29 20:50:58.415132: Epoch 11
2025-11-29 20:50:58.420293: Current learning rate: 0.0099
2025-11-29 20:54:17.394990: train_loss -0.7724
2025-11-29 20:54:17.423883: val_loss -0.6513
2025-11-29 20:54:17.432678: Pseudo dice [np.float32(0.8449), np.float32(0.8771)]
2025-11-29 20:54:17.437932: Epoch time: 198.99 s
2025-11-29 20:54:17.441824: Yayy! New best EMA pseudo Dice: 0.6718999743461609
2025-11-29 20:54:19.254193: 
2025-11-29 20:54:19.293773: Epoch 12
2025-11-29 20:54:19.298088: Current learning rate: 0.00989
2025-11-29 20:57:38.350816: train_loss -0.7819
2025-11-29 20:57:38.356487: val_loss -0.6149
2025-11-29 20:57:38.359231: Pseudo dice [np.float32(0.7888), np.float32(0.8697)]
2025-11-29 20:57:38.362037: Epoch time: 199.1 s
2025-11-29 20:57:38.364604: Yayy! New best EMA pseudo Dice: 0.6876999735832214
2025-11-29 20:57:40.073102: 
2025-11-29 20:57:40.076340: Epoch 13
2025-11-29 20:57:40.079526: Current learning rate: 0.00988
2025-11-29 21:00:59.049144: train_loss -0.7916
2025-11-29 21:00:59.054304: val_loss -0.5859
2025-11-29 21:00:59.062929: Pseudo dice [np.float32(0.7893), np.float32(0.8681)]
2025-11-29 21:00:59.067559: Epoch time: 198.98 s
2025-11-29 21:00:59.070446: Yayy! New best EMA pseudo Dice: 0.7017999887466431
2025-11-29 21:01:01.156624: 
2025-11-29 21:01:01.159891: Epoch 14
2025-11-29 21:01:01.163070: Current learning rate: 0.00987
2025-11-29 21:04:20.126796: train_loss -0.8094
2025-11-29 21:04:20.132472: val_loss -0.6266
2025-11-29 21:04:20.135388: Pseudo dice [np.float32(0.8297), np.float32(0.8771)]
2025-11-29 21:04:20.138569: Epoch time: 198.97 s
2025-11-29 21:04:20.141295: Yayy! New best EMA pseudo Dice: 0.7168999910354614
2025-11-29 21:04:22.075741: 
2025-11-29 21:04:22.078323: Epoch 15
2025-11-29 21:04:22.082282: Current learning rate: 0.00986
2025-11-29 21:07:41.052452: train_loss -0.8147
2025-11-29 21:07:41.071414: val_loss -0.5006
2025-11-29 21:07:41.074525: Pseudo dice [np.float32(0.7532), np.float32(0.8667)]
2025-11-29 21:07:41.077955: Epoch time: 198.98 s
2025-11-29 21:07:41.080495: Yayy! New best EMA pseudo Dice: 0.7261999845504761
2025-11-29 21:07:42.878814: 
2025-11-29 21:07:42.900351: Epoch 16
2025-11-29 21:07:42.915075: Current learning rate: 0.00986
2025-11-29 21:11:01.926592: train_loss -0.8066
2025-11-29 21:11:01.931475: val_loss -0.5457
2025-11-29 21:11:01.934488: Pseudo dice [np.float32(0.7608), np.float32(0.8731)]
2025-11-29 21:11:01.937585: Epoch time: 199.05 s
2025-11-29 21:11:01.940455: Yayy! New best EMA pseudo Dice: 0.7353000044822693
2025-11-29 21:11:03.909585: 
2025-11-29 21:11:03.913720: Epoch 17
2025-11-29 21:11:03.917652: Current learning rate: 0.00985
2025-11-29 21:14:22.888525: train_loss -0.8203
2025-11-29 21:14:22.893576: val_loss -0.5274
2025-11-29 21:14:22.897078: Pseudo dice [np.float32(0.7677), np.float32(0.8687)]
2025-11-29 21:14:22.899363: Epoch time: 198.98 s
2025-11-29 21:14:22.911911: Yayy! New best EMA pseudo Dice: 0.7436000108718872
2025-11-29 21:14:24.711447: 
2025-11-29 21:14:24.714666: Epoch 18
2025-11-29 21:14:24.718937: Current learning rate: 0.00984
2025-11-29 21:17:43.742510: train_loss -0.8191
2025-11-29 21:17:43.747613: val_loss -0.6679
2025-11-29 21:17:43.751247: Pseudo dice [np.float32(0.8819), np.float32(0.8987)]
2025-11-29 21:17:43.753478: Epoch time: 199.03 s
2025-11-29 21:17:43.756480: Yayy! New best EMA pseudo Dice: 0.7583000063896179
2025-11-29 21:17:45.577082: 
2025-11-29 21:17:45.580050: Epoch 19
2025-11-29 21:17:45.583560: Current learning rate: 0.00983
2025-11-29 21:21:04.571603: train_loss -0.8195
2025-11-29 21:21:04.577249: val_loss -0.5753
2025-11-29 21:21:04.580158: Pseudo dice [np.float32(0.8043), np.float32(0.8768)]
2025-11-29 21:21:04.582746: Epoch time: 199.0 s
2025-11-29 21:21:04.585122: Yayy! New best EMA pseudo Dice: 0.7664999961853027
2025-11-29 21:21:06.318428: 
2025-11-29 21:21:06.321776: Epoch 20
2025-11-29 21:21:06.326605: Current learning rate: 0.00982
2025-11-29 21:24:25.354926: train_loss -0.8284
2025-11-29 21:24:25.359102: val_loss -0.5943
2025-11-29 21:24:25.361523: Pseudo dice [np.float32(0.8182), np.float32(0.8809)]
2025-11-29 21:24:25.364170: Epoch time: 199.04 s
2025-11-29 21:24:25.366777: Yayy! New best EMA pseudo Dice: 0.7748000025749207
2025-11-29 21:24:27.417740: 
2025-11-29 21:24:27.421149: Epoch 21
2025-11-29 21:24:27.424043: Current learning rate: 0.00981
2025-11-29 21:27:46.401115: train_loss -0.8308
2025-11-29 21:27:46.413929: val_loss -0.6761
2025-11-29 21:27:46.416632: Pseudo dice [np.float32(0.8474), np.float32(0.8855)]
2025-11-29 21:27:46.419368: Epoch time: 198.98 s
2025-11-29 21:27:46.422907: Yayy! New best EMA pseudo Dice: 0.7839999794960022
2025-11-29 21:27:48.801739: 
2025-11-29 21:27:48.813956: Epoch 22
2025-11-29 21:27:48.818589: Current learning rate: 0.0098
2025-11-29 21:31:07.877056: train_loss -0.8282
2025-11-29 21:31:07.882184: val_loss -0.6458
2025-11-29 21:31:07.886405: Pseudo dice [np.float32(0.8607), np.float32(0.8962)]
2025-11-29 21:31:07.888781: Epoch time: 199.08 s
2025-11-29 21:31:07.891407: Yayy! New best EMA pseudo Dice: 0.79339998960495
2025-11-29 21:31:09.814996: 
2025-11-29 21:31:09.819533: Epoch 23
2025-11-29 21:31:09.823371: Current learning rate: 0.00979
2025-11-29 21:34:28.818753: train_loss -0.8217
2025-11-29 21:34:28.823276: val_loss -0.6398
2025-11-29 21:34:28.826033: Pseudo dice [np.float32(0.8281), np.float32(0.8872)]
2025-11-29 21:34:28.828835: Epoch time: 199.0 s
2025-11-29 21:34:28.831563: Yayy! New best EMA pseudo Dice: 0.7997999787330627
2025-11-29 21:34:30.753932: 
2025-11-29 21:34:30.756993: Epoch 24
2025-11-29 21:34:30.759987: Current learning rate: 0.00978
2025-11-29 21:37:49.794711: train_loss -0.8479
2025-11-29 21:37:49.799497: val_loss -0.6215
2025-11-29 21:37:49.808511: Pseudo dice [np.float32(0.828), np.float32(0.884)]
2025-11-29 21:37:49.811640: Epoch time: 199.04 s
2025-11-29 21:37:49.814236: Yayy! New best EMA pseudo Dice: 0.8054999709129333
2025-11-29 21:37:51.541132: 
2025-11-29 21:37:51.543905: Epoch 25
2025-11-29 21:37:51.548539: Current learning rate: 0.00977
2025-11-29 21:41:10.625503: train_loss -0.8499
2025-11-29 21:41:10.631395: val_loss -0.6264
2025-11-29 21:41:10.634251: Pseudo dice [np.float32(0.829), np.float32(0.8914)]
2025-11-29 21:41:10.636781: Epoch time: 199.09 s
2025-11-29 21:41:10.639332: Yayy! New best EMA pseudo Dice: 0.8108999729156494
2025-11-29 21:41:12.994165: 
2025-11-29 21:41:12.997963: Epoch 26
2025-11-29 21:41:13.009350: Current learning rate: 0.00977
2025-11-29 21:44:32.109056: train_loss -0.8558
2025-11-29 21:44:32.128210: val_loss -0.5488
2025-11-29 21:44:32.162371: Pseudo dice [np.float32(0.805), np.float32(0.8855)]
2025-11-29 21:44:32.165062: Epoch time: 199.12 s
2025-11-29 21:44:32.167664: Yayy! New best EMA pseudo Dice: 0.8144000172615051
2025-11-29 21:44:34.053303: 
2025-11-29 21:44:34.068714: Epoch 27
2025-11-29 21:44:34.073545: Current learning rate: 0.00976
2025-11-29 21:47:53.143576: train_loss -0.8517
2025-11-29 21:47:53.148248: val_loss -0.6379
2025-11-29 21:47:53.151494: Pseudo dice [np.float32(0.8236), np.float32(0.8858)]
2025-11-29 21:47:53.154285: Epoch time: 199.09 s
2025-11-29 21:47:53.156995: Yayy! New best EMA pseudo Dice: 0.8184000253677368
2025-11-29 21:47:54.831553: 
2025-11-29 21:47:54.834662: Epoch 28
2025-11-29 21:47:54.838357: Current learning rate: 0.00975
2025-11-29 21:51:13.834882: train_loss -0.8683
2025-11-29 21:51:13.839783: val_loss -0.6204
2025-11-29 21:51:13.843021: Pseudo dice [np.float32(0.792), np.float32(0.8875)]
2025-11-29 21:51:13.845560: Epoch time: 199.0 s
2025-11-29 21:51:13.848151: Yayy! New best EMA pseudo Dice: 0.8205000162124634
2025-11-29 21:51:15.573243: 
2025-11-29 21:51:15.576705: Epoch 29
2025-11-29 21:51:15.579356: Current learning rate: 0.00974
2025-11-29 21:54:34.674146: train_loss -0.8496
2025-11-29 21:54:34.680400: val_loss -0.6139
2025-11-29 21:54:34.684139: Pseudo dice [np.float32(0.8145), np.float32(0.8847)]
2025-11-29 21:54:34.687771: Epoch time: 199.1 s
2025-11-29 21:54:34.691287: Yayy! New best EMA pseudo Dice: 0.8234000205993652
2025-11-29 21:54:36.624302: 
2025-11-29 21:54:36.628024: Epoch 30
2025-11-29 21:54:36.630784: Current learning rate: 0.00973
2025-11-29 21:57:55.760747: train_loss -0.8615
2025-11-29 21:57:55.770105: val_loss -0.5503
2025-11-29 21:57:55.773512: Pseudo dice [np.float32(0.7584), np.float32(0.8793)]
2025-11-29 21:57:55.789376: Epoch time: 199.14 s
2025-11-29 21:57:56.827318: 
2025-11-29 21:57:56.832676: Epoch 31
2025-11-29 21:57:56.835766: Current learning rate: 0.00972
2025-11-29 22:01:15.871622: train_loss -0.8715
2025-11-29 22:01:15.876428: val_loss -0.6404
2025-11-29 22:01:15.878968: Pseudo dice [np.float32(0.824), np.float32(0.8901)]
2025-11-29 22:01:15.881306: Epoch time: 199.05 s
2025-11-29 22:01:15.884076: Yayy! New best EMA pseudo Dice: 0.8263999819755554
2025-11-29 22:01:17.599489: 
2025-11-29 22:01:17.611997: Epoch 32
2025-11-29 22:01:17.614670: Current learning rate: 0.00971
2025-11-29 22:04:36.645562: train_loss -0.867
2025-11-29 22:04:36.649745: val_loss -0.7322
2025-11-29 22:04:36.652865: Pseudo dice [np.float32(0.9171), np.float32(0.9221)]
2025-11-29 22:04:36.656287: Epoch time: 199.05 s
2025-11-29 22:04:36.658665: Yayy! New best EMA pseudo Dice: 0.8356999754905701
2025-11-29 22:04:38.754894: 
2025-11-29 22:04:38.758827: Epoch 33
2025-11-29 22:04:38.761446: Current learning rate: 0.0097
2025-11-29 22:07:57.813431: train_loss -0.8662
2025-11-29 22:07:57.817966: val_loss -0.6039
2025-11-29 22:07:57.820426: Pseudo dice [np.float32(0.821), np.float32(0.8888)]
2025-11-29 22:07:57.823272: Epoch time: 199.06 s
2025-11-29 22:07:57.826202: Yayy! New best EMA pseudo Dice: 0.8375999927520752
2025-11-29 22:07:59.709914: 
2025-11-29 22:07:59.713406: Epoch 34
2025-11-29 22:07:59.716704: Current learning rate: 0.00969
2025-11-29 22:11:18.703396: train_loss -0.8746
2025-11-29 22:11:18.712842: val_loss -0.5925
2025-11-29 22:11:18.715497: Pseudo dice [np.float32(0.8132), np.float32(0.8955)]
2025-11-29 22:11:18.718478: Epoch time: 198.99 s
2025-11-29 22:11:18.724938: Yayy! New best EMA pseudo Dice: 0.8392999768257141
2025-11-29 22:11:20.831145: 
2025-11-29 22:11:20.834027: Epoch 35
2025-11-29 22:11:20.838504: Current learning rate: 0.00968
2025-11-29 22:14:39.871028: train_loss -0.8779
2025-11-29 22:14:39.876298: val_loss -0.6528
2025-11-29 22:14:39.879009: Pseudo dice [np.float32(0.8687), np.float32(0.8996)]
2025-11-29 22:14:39.881963: Epoch time: 199.04 s
2025-11-29 22:14:39.884737: Yayy! New best EMA pseudo Dice: 0.8438000082969666
2025-11-29 22:14:41.795676: 
2025-11-29 22:14:41.799143: Epoch 36
2025-11-29 22:14:41.811987: Current learning rate: 0.00968
2025-11-29 22:18:00.838407: train_loss -0.8708
2025-11-29 22:18:00.843387: val_loss -0.5722
2025-11-29 22:18:00.846165: Pseudo dice [np.float32(0.7853), np.float32(0.8945)]
2025-11-29 22:18:00.848653: Epoch time: 199.04 s
2025-11-29 22:18:01.709849: 
2025-11-29 22:18:01.715546: Epoch 37
2025-11-29 22:18:01.718637: Current learning rate: 0.00967
2025-11-29 22:21:20.670264: train_loss -0.8692
2025-11-29 22:21:20.677120: val_loss -0.5943
2025-11-29 22:21:20.679662: Pseudo dice [np.float32(0.7978), np.float32(0.8912)]
2025-11-29 22:21:20.682498: Epoch time: 198.96 s
2025-11-29 22:21:21.537407: 
2025-11-29 22:21:21.540490: Epoch 38
2025-11-29 22:21:21.545230: Current learning rate: 0.00966
2025-11-29 22:24:40.473703: train_loss -0.8818
2025-11-29 22:24:40.478355: val_loss -0.608
2025-11-29 22:24:40.481884: Pseudo dice [np.float32(0.8268), np.float32(0.8976)]
2025-11-29 22:24:40.484822: Epoch time: 198.94 s
2025-11-29 22:24:40.488134: Yayy! New best EMA pseudo Dice: 0.8453999757766724
2025-11-29 22:24:42.152587: 
2025-11-29 22:24:42.158989: Epoch 39
2025-11-29 22:24:42.161571: Current learning rate: 0.00965
2025-11-29 22:28:01.167321: train_loss -0.8585
2025-11-29 22:28:01.186534: val_loss -0.7241
2025-11-29 22:28:01.191753: Pseudo dice [np.float32(0.8749), np.float32(0.9028)]
2025-11-29 22:28:01.197958: Epoch time: 199.02 s
2025-11-29 22:28:01.210224: Yayy! New best EMA pseudo Dice: 0.8496999740600586
2025-11-29 22:28:03.358079: 
2025-11-29 22:28:03.364416: Epoch 40
2025-11-29 22:28:03.370265: Current learning rate: 0.00964
2025-11-29 22:31:22.348163: train_loss -0.8678
2025-11-29 22:31:22.352681: val_loss -0.5895
2025-11-29 22:31:22.356612: Pseudo dice [np.float32(0.8007), np.float32(0.8891)]
2025-11-29 22:31:22.359735: Epoch time: 198.99 s
2025-11-29 22:31:23.390875: 
2025-11-29 22:31:23.396239: Epoch 41
2025-11-29 22:31:23.398827: Current learning rate: 0.00963
2025-11-29 22:34:42.434422: train_loss -0.8557
2025-11-29 22:34:42.439887: val_loss -0.604
2025-11-29 22:34:42.442692: Pseudo dice [np.float32(0.7952), np.float32(0.8827)]
2025-11-29 22:34:42.445151: Epoch time: 199.04 s
2025-11-29 22:34:43.482634: 
2025-11-29 22:34:43.505044: Epoch 42
2025-11-29 22:34:43.517105: Current learning rate: 0.00962
2025-11-29 22:38:02.527562: train_loss -0.8732
2025-11-29 22:38:02.533265: val_loss -0.597
2025-11-29 22:38:02.536489: Pseudo dice [np.float32(0.7891), np.float32(0.8854)]
2025-11-29 22:38:02.539569: Epoch time: 199.05 s
2025-11-29 22:38:03.537825: 
2025-11-29 22:38:03.547230: Epoch 43
2025-11-29 22:38:03.551629: Current learning rate: 0.00961
2025-11-29 22:41:22.554795: train_loss -0.8855
2025-11-29 22:41:22.559401: val_loss -0.6319
2025-11-29 22:41:22.562435: Pseudo dice [np.float32(0.8311), np.float32(0.8966)]
2025-11-29 22:41:22.564563: Epoch time: 199.02 s
2025-11-29 22:41:23.410521: 
2025-11-29 22:41:23.413598: Epoch 44
2025-11-29 22:41:23.416388: Current learning rate: 0.0096
2025-11-29 22:44:42.521646: train_loss -0.885
2025-11-29 22:44:42.526218: val_loss -0.6764
2025-11-29 22:44:42.528611: Pseudo dice [np.float32(0.8732), np.float32(0.9)]
2025-11-29 22:44:42.531358: Epoch time: 199.11 s
2025-11-29 22:44:42.533667: Yayy! New best EMA pseudo Dice: 0.8525999784469604
2025-11-29 22:44:44.310481: 
2025-11-29 22:44:44.313636: Epoch 45
2025-11-29 22:44:44.318155: Current learning rate: 0.00959
2025-11-29 22:48:03.438623: train_loss -0.8853
2025-11-29 22:48:03.447741: val_loss -0.5554
2025-11-29 22:48:03.452764: Pseudo dice [np.float32(0.7965), np.float32(0.8866)]
2025-11-29 22:48:03.456067: Epoch time: 199.13 s
2025-11-29 22:48:05.071726: 
2025-11-29 22:48:05.075859: Epoch 46
2025-11-29 22:48:05.078935: Current learning rate: 0.00959
2025-11-29 22:51:24.210245: train_loss -0.8844
2025-11-29 22:51:24.218520: val_loss -0.595
2025-11-29 22:51:24.223498: Pseudo dice [np.float32(0.7846), np.float32(0.8917)]
2025-11-29 22:51:24.230050: Epoch time: 199.14 s
2025-11-29 22:51:25.262754: 
2025-11-29 22:51:25.290644: Epoch 47
2025-11-29 22:51:25.300354: Current learning rate: 0.00958
2025-11-29 22:54:44.409427: train_loss -0.8926
2025-11-29 22:54:44.432083: val_loss -0.6959
2025-11-29 22:54:44.435931: Pseudo dice [np.float32(0.8735), np.float32(0.9108)]
2025-11-29 22:54:44.438652: Epoch time: 199.15 s
2025-11-29 22:54:44.440784: Yayy! New best EMA pseudo Dice: 0.8543000221252441
2025-11-29 22:54:46.339356: 
2025-11-29 22:54:46.341988: Epoch 48
2025-11-29 22:54:46.344437: Current learning rate: 0.00957
2025-11-29 22:58:05.442557: train_loss -0.8925
2025-11-29 22:58:05.447194: val_loss -0.6286
2025-11-29 22:58:05.449452: Pseudo dice [np.float32(0.8156), np.float32(0.8983)]
2025-11-29 22:58:05.452255: Epoch time: 199.1 s
2025-11-29 22:58:05.454810: Yayy! New best EMA pseudo Dice: 0.8546000123023987
2025-11-29 22:58:07.209541: 
2025-11-29 22:58:07.213649: Epoch 49
2025-11-29 22:58:07.217190: Current learning rate: 0.00956
2025-11-29 23:01:26.253916: train_loss -0.8793
2025-11-29 23:01:26.259480: val_loss -0.6187
2025-11-29 23:01:26.262597: Pseudo dice [np.float32(0.8064), np.float32(0.8801)]
2025-11-29 23:01:26.267916: Epoch time: 199.05 s
2025-11-29 23:01:27.975852: 
2025-11-29 23:01:27.978550: Epoch 50
2025-11-29 23:01:27.981219: Current learning rate: 0.00955
2025-11-29 23:04:47.017068: train_loss -0.8708
2025-11-29 23:04:47.022206: val_loss -0.5599
2025-11-29 23:04:47.026372: Pseudo dice [np.float32(0.7257), np.float32(0.8703)]
2025-11-29 23:04:47.030371: Epoch time: 199.04 s
2025-11-29 23:04:47.877651: 
2025-11-29 23:04:47.880964: Epoch 51
2025-11-29 23:04:47.883707: Current learning rate: 0.00954
2025-11-29 23:08:06.848507: train_loss -0.876
2025-11-29 23:08:06.853992: val_loss -0.6631
2025-11-29 23:08:06.857210: Pseudo dice [np.float32(0.8782), np.float32(0.9147)]
2025-11-29 23:08:06.859556: Epoch time: 198.97 s
2025-11-29 23:08:07.698314: 
2025-11-29 23:08:07.708827: Epoch 52
2025-11-29 23:08:07.712698: Current learning rate: 0.00953
2025-11-29 23:11:26.787674: train_loss -0.8935
2025-11-29 23:11:26.798144: val_loss -0.6307
2025-11-29 23:11:26.807941: Pseudo dice [np.float32(0.8264), np.float32(0.8999)]
2025-11-29 23:11:26.811001: Epoch time: 199.09 s
2025-11-29 23:11:27.676005: 
2025-11-29 23:11:27.679812: Epoch 53
2025-11-29 23:11:27.683208: Current learning rate: 0.00952
2025-11-29 23:14:46.746184: train_loss -0.8958
2025-11-29 23:14:46.751186: val_loss -0.6209
2025-11-29 23:14:46.754966: Pseudo dice [np.float32(0.844), np.float32(0.9099)]
2025-11-29 23:14:46.758396: Epoch time: 199.07 s
2025-11-29 23:14:46.760908: Yayy! New best EMA pseudo Dice: 0.8561000227928162
2025-11-29 23:14:48.497793: 
2025-11-29 23:14:48.509358: Epoch 54
2025-11-29 23:14:48.512820: Current learning rate: 0.00951
2025-11-29 23:18:07.564495: train_loss -0.9028
2025-11-29 23:18:07.573993: val_loss -0.622
2025-11-29 23:18:07.577463: Pseudo dice [np.float32(0.7962), np.float32(0.893)]
2025-11-29 23:18:07.580038: Epoch time: 199.07 s
2025-11-29 23:18:08.423239: 
2025-11-29 23:18:08.427651: Epoch 55
2025-11-29 23:18:08.430143: Current learning rate: 0.0095
2025-11-29 23:21:27.402911: train_loss -0.9074
2025-11-29 23:21:27.413631: val_loss -0.6481
2025-11-29 23:21:27.416612: Pseudo dice [np.float32(0.8525), np.float32(0.9034)]
2025-11-29 23:21:27.419446: Epoch time: 198.98 s
2025-11-29 23:21:27.422764: Yayy! New best EMA pseudo Dice: 0.8572999835014343
2025-11-29 23:21:29.397162: 
2025-11-29 23:21:29.407516: Epoch 56
2025-11-29 23:21:29.411133: Current learning rate: 0.00949
2025-11-29 23:24:48.439137: train_loss -0.901
2025-11-29 23:24:48.444219: val_loss -0.6808
2025-11-29 23:24:48.447461: Pseudo dice [np.float32(0.8599), np.float32(0.91)]
2025-11-29 23:24:48.449810: Epoch time: 199.04 s
2025-11-29 23:24:48.452914: Yayy! New best EMA pseudo Dice: 0.8600000143051147
2025-11-29 23:24:50.218766: 
2025-11-29 23:24:50.221963: Epoch 57
2025-11-29 23:24:50.224531: Current learning rate: 0.00949
2025-11-29 23:28:09.229392: train_loss -0.9004
2025-11-29 23:28:09.235189: val_loss -0.6842
2025-11-29 23:28:09.238229: Pseudo dice [np.float32(0.866), np.float32(0.905)]
2025-11-29 23:28:09.240284: Epoch time: 199.01 s
2025-11-29 23:28:09.242866: Yayy! New best EMA pseudo Dice: 0.8626000285148621
2025-11-29 23:28:11.242167: 
2025-11-29 23:28:11.245912: Epoch 58
2025-11-29 23:28:11.248921: Current learning rate: 0.00948
2025-11-29 23:31:30.254097: train_loss -0.9002
2025-11-29 23:31:30.259723: val_loss -0.6608
2025-11-29 23:31:30.262870: Pseudo dice [np.float32(0.8416), np.float32(0.9035)]
2025-11-29 23:31:30.265942: Epoch time: 199.01 s
2025-11-29 23:31:30.268590: Yayy! New best EMA pseudo Dice: 0.8636000156402588
2025-11-29 23:31:32.161769: 
2025-11-29 23:31:32.167075: Epoch 59
2025-11-29 23:31:32.170296: Current learning rate: 0.00947
2025-11-29 23:34:51.146767: train_loss -0.9021
2025-11-29 23:34:51.152503: val_loss -0.6389
2025-11-29 23:34:51.155184: Pseudo dice [np.float32(0.8366), np.float32(0.8913)]
2025-11-29 23:34:51.158370: Epoch time: 198.99 s
2025-11-29 23:34:51.160966: Yayy! New best EMA pseudo Dice: 0.8636000156402588
2025-11-29 23:34:52.943107: 
2025-11-29 23:34:52.945806: Epoch 60
2025-11-29 23:34:52.950600: Current learning rate: 0.00946
2025-11-29 23:38:11.909587: train_loss -0.9032
2025-11-29 23:38:11.916301: val_loss -0.6656
2025-11-29 23:38:11.920630: Pseudo dice [np.float32(0.845), np.float32(0.9013)]
2025-11-29 23:38:11.924508: Epoch time: 198.97 s
2025-11-29 23:38:11.938544: Yayy! New best EMA pseudo Dice: 0.8646000027656555
2025-11-29 23:38:14.033021: 
2025-11-29 23:38:14.037472: Epoch 61
2025-11-29 23:38:14.040348: Current learning rate: 0.00945
2025-11-29 23:41:33.008463: train_loss -0.8907
2025-11-29 23:41:33.014650: val_loss -0.6263
2025-11-29 23:41:33.017847: Pseudo dice [np.float32(0.8404), np.float32(0.9036)]
2025-11-29 23:41:33.020233: Epoch time: 198.98 s
2025-11-29 23:41:33.023013: Yayy! New best EMA pseudo Dice: 0.8652999997138977
2025-11-29 23:41:34.976847: 
2025-11-29 23:41:34.981080: Epoch 62
2025-11-29 23:41:34.984776: Current learning rate: 0.00944
2025-11-29 23:44:54.043318: train_loss -0.8793
2025-11-29 23:44:54.047889: val_loss -0.7245
2025-11-29 23:44:54.051064: Pseudo dice [np.float32(0.8861), np.float32(0.9103)]
2025-11-29 23:44:54.053609: Epoch time: 199.07 s
2025-11-29 23:44:54.056089: Yayy! New best EMA pseudo Dice: 0.8686000108718872
2025-11-29 23:44:56.040716: 
2025-11-29 23:44:56.043595: Epoch 63
2025-11-29 23:44:56.047040: Current learning rate: 0.00943
2025-11-29 23:48:15.100107: train_loss -0.8781
2025-11-29 23:48:15.113937: val_loss -0.685
2025-11-29 23:48:15.116854: Pseudo dice [np.float32(0.868), np.float32(0.9005)]
2025-11-29 23:48:15.119206: Epoch time: 199.06 s
2025-11-29 23:48:15.122792: Yayy! New best EMA pseudo Dice: 0.870199978351593
2025-11-29 23:48:16.947604: 
2025-11-29 23:48:16.950391: Epoch 64
2025-11-29 23:48:16.953091: Current learning rate: 0.00942
2025-11-29 23:51:36.042062: train_loss -0.8872
2025-11-29 23:51:36.049716: val_loss -0.6669
2025-11-29 23:51:36.053329: Pseudo dice [np.float32(0.8801), np.float32(0.9139)]
2025-11-29 23:51:36.059526: Epoch time: 199.1 s
2025-11-29 23:51:36.063538: Yayy! New best EMA pseudo Dice: 0.8729000091552734
2025-11-29 23:51:38.137071: 
2025-11-29 23:51:38.142277: Epoch 65
2025-11-29 23:51:38.147919: Current learning rate: 0.00941
2025-11-29 23:54:57.275313: train_loss -0.8861
2025-11-29 23:54:57.282563: val_loss -0.6186
2025-11-29 23:54:57.286423: Pseudo dice [np.float32(0.8437), np.float32(0.8919)]
2025-11-29 23:54:57.291544: Epoch time: 199.14 s
2025-11-29 23:54:58.225621: 
2025-11-29 23:54:58.230712: Epoch 66
2025-11-29 23:54:58.234353: Current learning rate: 0.0094
2025-11-29 23:58:17.290589: train_loss -0.8916
2025-11-29 23:58:17.295236: val_loss -0.7259
2025-11-29 23:58:17.299274: Pseudo dice [np.float32(0.8798), np.float32(0.9112)]
2025-11-29 23:58:17.310251: Epoch time: 199.07 s
2025-11-29 23:58:17.318687: Yayy! New best EMA pseudo Dice: 0.8747000098228455
2025-11-29 23:58:19.350981: 
2025-11-29 23:58:19.355926: Epoch 67
2025-11-29 23:58:19.360308: Current learning rate: 0.00939
2025-11-30 00:01:38.302208: train_loss -0.8968
2025-11-30 00:01:38.312844: val_loss -0.6316
2025-11-30 00:01:38.315735: Pseudo dice [np.float32(0.8122), np.float32(0.8921)]
2025-11-30 00:01:38.318714: Epoch time: 198.95 s
2025-11-30 00:01:39.184416: 
2025-11-30 00:01:39.189520: Epoch 68
2025-11-30 00:01:39.194477: Current learning rate: 0.00939
2025-11-30 00:04:58.206979: train_loss -0.9039
2025-11-30 00:04:58.218753: val_loss -0.7177
2025-11-30 00:04:58.221848: Pseudo dice [np.float32(0.8652), np.float32(0.9068)]
2025-11-30 00:04:58.225013: Epoch time: 199.02 s
2025-11-30 00:04:59.091258: 
2025-11-30 00:04:59.098091: Epoch 69
2025-11-30 00:04:59.112992: Current learning rate: 0.00938
2025-11-30 00:08:18.136766: train_loss -0.9052
2025-11-30 00:08:18.141273: val_loss -0.7437
2025-11-30 00:08:18.144505: Pseudo dice [np.float32(0.904), np.float32(0.918)]
2025-11-30 00:08:18.147472: Epoch time: 199.05 s
2025-11-30 00:08:18.149667: Yayy! New best EMA pseudo Dice: 0.8774999976158142
2025-11-30 00:08:20.479268: 
2025-11-30 00:08:20.482886: Epoch 70
2025-11-30 00:08:20.486374: Current learning rate: 0.00937
2025-11-30 00:11:39.537673: train_loss -0.9042
2025-11-30 00:11:39.542878: val_loss -0.6338
2025-11-30 00:11:39.545432: Pseudo dice [np.float32(0.7718), np.float32(0.8824)]
2025-11-30 00:11:39.548579: Epoch time: 199.06 s
2025-11-30 00:11:40.533414: 
2025-11-30 00:11:40.542315: Epoch 71
2025-11-30 00:11:40.553484: Current learning rate: 0.00936
2025-11-30 00:14:59.568686: train_loss -0.9074
2025-11-30 00:14:59.573698: val_loss -0.7355
2025-11-30 00:14:59.577790: Pseudo dice [np.float32(0.9051), np.float32(0.9214)]
2025-11-30 00:14:59.581067: Epoch time: 199.04 s
2025-11-30 00:15:00.445002: 
2025-11-30 00:15:00.448147: Epoch 72
2025-11-30 00:15:00.452582: Current learning rate: 0.00935
2025-11-30 00:18:19.522369: train_loss -0.9109
2025-11-30 00:18:19.528866: val_loss -0.7026
2025-11-30 00:18:19.532652: Pseudo dice [np.float32(0.8793), np.float32(0.9082)]
2025-11-30 00:18:19.537537: Epoch time: 199.08 s
2025-11-30 00:18:19.540389: Yayy! New best EMA pseudo Dice: 0.8783000111579895
2025-11-30 00:18:21.389816: 
2025-11-30 00:18:21.394509: Epoch 73
2025-11-30 00:18:21.399950: Current learning rate: 0.00934
2025-11-30 00:21:40.413686: train_loss -0.9095
2025-11-30 00:21:40.418681: val_loss -0.7349
2025-11-30 00:21:40.421612: Pseudo dice [np.float32(0.8786), np.float32(0.9068)]
2025-11-30 00:21:40.424208: Epoch time: 199.02 s
2025-11-30 00:21:40.426999: Yayy! New best EMA pseudo Dice: 0.8797000050544739
2025-11-30 00:21:42.312595: 
2025-11-30 00:21:42.315617: Epoch 74
2025-11-30 00:21:42.320433: Current learning rate: 0.00933
2025-11-30 00:25:01.607698: train_loss -0.8884
2025-11-30 00:25:01.615528: val_loss -0.7037
2025-11-30 00:25:01.618654: Pseudo dice [np.float32(0.844), np.float32(0.8931)]
2025-11-30 00:25:01.621306: Epoch time: 199.3 s
2025-11-30 00:25:02.480736: 
2025-11-30 00:25:02.485322: Epoch 75
2025-11-30 00:25:02.488253: Current learning rate: 0.00932
2025-11-30 00:28:21.773758: train_loss -0.8934
2025-11-30 00:28:21.781441: val_loss -0.6762
2025-11-30 00:28:21.785296: Pseudo dice [np.float32(0.85), np.float32(0.9026)]
2025-11-30 00:28:21.788487: Epoch time: 199.29 s
2025-11-30 00:28:22.786855: 
2025-11-30 00:28:22.794563: Epoch 76
2025-11-30 00:28:22.800344: Current learning rate: 0.00931
2025-11-30 00:31:42.151217: train_loss -0.908
2025-11-30 00:31:42.159303: val_loss -0.6165
2025-11-30 00:31:42.164785: Pseudo dice [np.float32(0.8012), np.float32(0.8954)]
2025-11-30 00:31:42.169263: Epoch time: 199.37 s
2025-11-30 00:31:43.053887: 
2025-11-30 00:31:43.058663: Epoch 77
2025-11-30 00:31:43.063493: Current learning rate: 0.0093
2025-11-30 00:35:02.368027: train_loss -0.8969
2025-11-30 00:35:02.373497: val_loss -0.6731
2025-11-30 00:35:02.376312: Pseudo dice [np.float32(0.8573), np.float32(0.8999)]
2025-11-30 00:35:02.379544: Epoch time: 199.32 s
2025-11-30 00:35:03.259573: 
2025-11-30 00:35:03.263467: Epoch 78
2025-11-30 00:35:03.266685: Current learning rate: 0.0093
2025-11-30 00:38:22.553247: train_loss -0.9089
2025-11-30 00:38:22.561863: val_loss -0.654
2025-11-30 00:38:22.567182: Pseudo dice [np.float32(0.8496), np.float32(0.9026)]
2025-11-30 00:38:22.572202: Epoch time: 199.29 s
2025-11-30 00:38:23.468997: 
2025-11-30 00:38:23.473567: Epoch 79
2025-11-30 00:38:23.478207: Current learning rate: 0.00929
2025-11-30 00:41:42.776830: train_loss -0.9126
2025-11-30 00:41:42.785298: val_loss -0.5845
2025-11-30 00:41:42.790403: Pseudo dice [np.float32(0.8231), np.float32(0.9005)]
2025-11-30 00:41:42.795575: Epoch time: 199.31 s
2025-11-30 00:41:43.891359: 
2025-11-30 00:41:43.899312: Epoch 80
2025-11-30 00:41:43.912663: Current learning rate: 0.00928
2025-11-30 00:45:03.258709: train_loss -0.9079
2025-11-30 00:45:03.266147: val_loss -0.7565
2025-11-30 00:45:03.270549: Pseudo dice [np.float32(0.8944), np.float32(0.9183)]
2025-11-30 00:45:03.275343: Epoch time: 199.37 s
2025-11-30 00:45:04.334672: 
2025-11-30 00:45:04.342339: Epoch 81
2025-11-30 00:45:04.346264: Current learning rate: 0.00927
2025-11-30 00:48:23.599125: train_loss -0.9076
2025-11-30 00:48:23.615263: val_loss -0.5955
2025-11-30 00:48:23.620072: Pseudo dice [np.float32(0.8305), np.float32(0.9008)]
2025-11-30 00:48:23.624646: Epoch time: 199.27 s
2025-11-30 00:48:25.309285: 
2025-11-30 00:48:25.314023: Epoch 82
2025-11-30 00:48:25.317081: Current learning rate: 0.00926
2025-11-30 00:51:44.561757: train_loss -0.9117
2025-11-30 00:51:44.569181: val_loss -0.605
2025-11-30 00:51:44.574630: Pseudo dice [np.float32(0.7948), np.float32(0.8935)]
2025-11-30 00:51:44.579300: Epoch time: 199.25 s
2025-11-30 00:51:45.440176: 
2025-11-30 00:51:45.446595: Epoch 83
2025-11-30 00:51:45.451524: Current learning rate: 0.00925
2025-11-30 00:55:04.734507: train_loss -0.9114
2025-11-30 00:55:04.746047: val_loss -0.6313
2025-11-30 00:55:04.750394: Pseudo dice [np.float32(0.8178), np.float32(0.8983)]
2025-11-30 00:55:04.753841: Epoch time: 199.3 s
2025-11-30 00:55:05.609114: 
2025-11-30 00:55:05.614506: Epoch 84
2025-11-30 00:55:05.618116: Current learning rate: 0.00924
2025-11-30 00:58:24.936983: train_loss -0.913
2025-11-30 00:58:24.943810: val_loss -0.6587
2025-11-30 00:58:24.947811: Pseudo dice [np.float32(0.8722), np.float32(0.9125)]
2025-11-30 00:58:24.953061: Epoch time: 199.33 s
2025-11-30 00:58:25.800164: 
2025-11-30 00:58:25.814647: Epoch 85
2025-11-30 00:58:25.819985: Current learning rate: 0.00923
2025-11-30 01:01:44.985219: train_loss -0.9123
2025-11-30 01:01:44.994195: val_loss -0.6146
2025-11-30 01:01:45.009138: Pseudo dice [np.float32(0.8567), np.float32(0.9074)]
2025-11-30 01:01:45.017689: Epoch time: 199.19 s
2025-11-30 01:01:45.899627: 
2025-11-30 01:01:45.912646: Epoch 86
2025-11-30 01:01:45.917669: Current learning rate: 0.00922
2025-11-30 01:05:05.177684: train_loss -0.9148
2025-11-30 01:05:05.190984: val_loss -0.6397
2025-11-30 01:05:05.197606: Pseudo dice [np.float32(0.8211), np.float32(0.9054)]
2025-11-30 01:05:05.211665: Epoch time: 199.28 s
2025-11-30 01:05:06.362211: 
2025-11-30 01:05:06.369729: Epoch 87
2025-11-30 01:05:06.374240: Current learning rate: 0.00921
2025-11-30 01:08:25.776152: train_loss -0.912
2025-11-30 01:08:25.788697: val_loss -0.6014
2025-11-30 01:08:25.795133: Pseudo dice [np.float32(0.8217), np.float32(0.899)]
2025-11-30 01:08:25.809620: Epoch time: 199.42 s
2025-11-30 01:08:26.931698: 
2025-11-30 01:08:26.945724: Epoch 88
2025-11-30 01:08:26.952347: Current learning rate: 0.0092
2025-11-30 01:11:46.228550: train_loss -0.9107
2025-11-30 01:11:46.236812: val_loss -0.607
2025-11-30 01:11:46.244907: Pseudo dice [np.float32(0.8215), np.float32(0.8948)]
2025-11-30 01:11:46.250669: Epoch time: 199.3 s
2025-11-30 01:11:47.259049: 
2025-11-30 01:11:47.266091: Epoch 89
2025-11-30 01:11:47.269717: Current learning rate: 0.0092
2025-11-30 01:15:06.396508: train_loss -0.9087
2025-11-30 01:15:06.417939: val_loss -0.5107
2025-11-30 01:15:06.425241: Pseudo dice [np.float32(0.7456), np.float32(0.8798)]
2025-11-30 01:15:06.432066: Epoch time: 199.14 s
2025-11-30 01:15:07.559555: 
2025-11-30 01:15:07.591506: Epoch 90
2025-11-30 01:15:07.596864: Current learning rate: 0.00919
2025-11-30 01:18:26.670131: train_loss -0.9054
2025-11-30 01:18:26.677567: val_loss -0.5297
2025-11-30 01:18:26.685229: Pseudo dice [np.float32(0.7568), np.float32(0.878)]
2025-11-30 01:18:26.690995: Epoch time: 199.11 s
2025-11-30 01:18:27.561886: 
2025-11-30 01:18:27.567369: Epoch 91
2025-11-30 01:18:27.573583: Current learning rate: 0.00918
2025-11-30 01:21:46.666532: train_loss -0.8943
2025-11-30 01:21:46.672995: val_loss -0.526
2025-11-30 01:21:46.678510: Pseudo dice [np.float32(0.7743), np.float32(0.8874)]
2025-11-30 01:21:46.681648: Epoch time: 199.11 s
2025-11-30 01:21:47.518522: 
2025-11-30 01:21:47.521096: Epoch 92
2025-11-30 01:21:47.524126: Current learning rate: 0.00917
2025-11-30 01:25:06.680322: train_loss -0.9089
2025-11-30 01:25:06.686638: val_loss -0.647
2025-11-30 01:25:06.690660: Pseudo dice [np.float32(0.8091), np.float32(0.8991)]
2025-11-30 01:25:06.695039: Epoch time: 199.16 s
2025-11-30 01:25:07.660940: 
2025-11-30 01:25:07.665111: Epoch 93
2025-11-30 01:25:07.669737: Current learning rate: 0.00916
2025-11-30 01:28:26.667023: train_loss -0.9083
2025-11-30 01:28:26.672339: val_loss -0.61
2025-11-30 01:28:26.675066: Pseudo dice [np.float32(0.8251), np.float32(0.8984)]
2025-11-30 01:28:26.678553: Epoch time: 199.01 s
2025-11-30 01:28:27.517274: 
2025-11-30 01:28:27.520984: Epoch 94
2025-11-30 01:28:27.523793: Current learning rate: 0.00915
2025-11-30 01:31:46.662664: train_loss -0.9128
2025-11-30 01:31:46.668000: val_loss -0.5966
2025-11-30 01:31:46.671053: Pseudo dice [np.float32(0.8192), np.float32(0.9026)]
2025-11-30 01:31:46.673661: Epoch time: 199.15 s
2025-11-30 01:31:48.032690: 
2025-11-30 01:31:48.037448: Epoch 95
2025-11-30 01:31:48.039967: Current learning rate: 0.00914
2025-11-30 01:35:07.044357: train_loss -0.9185
2025-11-30 01:35:07.053391: val_loss -0.6501
2025-11-30 01:35:07.056228: Pseudo dice [np.float32(0.8439), np.float32(0.9074)]
2025-11-30 01:35:07.058816: Epoch time: 199.01 s
2025-11-30 01:35:07.909418: 
2025-11-30 01:35:07.914436: Epoch 96
2025-11-30 01:35:07.919160: Current learning rate: 0.00913
2025-11-30 01:38:27.011479: train_loss -0.9123
2025-11-30 01:38:27.016256: val_loss -0.6455
2025-11-30 01:38:27.019006: Pseudo dice [np.float32(0.8571), np.float32(0.9054)]
2025-11-30 01:38:27.022039: Epoch time: 199.1 s
2025-11-30 01:38:27.873628: 
2025-11-30 01:38:27.879574: Epoch 97
2025-11-30 01:38:27.886615: Current learning rate: 0.00912
2025-11-30 01:41:46.987869: train_loss -0.9149
2025-11-30 01:41:46.996822: val_loss -0.7048
2025-11-30 01:41:47.009045: Pseudo dice [np.float32(0.8415), np.float32(0.9012)]
2025-11-30 01:41:47.013935: Epoch time: 199.12 s
2025-11-30 01:41:47.891799: 
2025-11-30 01:41:47.894974: Epoch 98
2025-11-30 01:41:47.898571: Current learning rate: 0.00911
2025-11-30 01:45:07.189111: train_loss -0.9153
2025-11-30 01:45:07.198184: val_loss -0.4975
2025-11-30 01:45:07.228718: Pseudo dice [np.float32(0.7464), np.float32(0.8838)]
2025-11-30 01:45:07.233154: Epoch time: 199.3 s
2025-11-30 01:45:08.116408: 
2025-11-30 01:45:08.121381: Epoch 99
2025-11-30 01:45:08.125729: Current learning rate: 0.0091
2025-11-30 01:48:27.357957: train_loss -0.9121
2025-11-30 01:48:27.365058: val_loss -0.6736
2025-11-30 01:48:27.369937: Pseudo dice [np.float32(0.8614), np.float32(0.9026)]
2025-11-30 01:48:27.374624: Epoch time: 199.24 s
2025-11-30 01:48:29.693481: 
2025-11-30 01:48:29.699833: Epoch 100
2025-11-30 01:48:29.713789: Current learning rate: 0.0091
2025-11-30 01:51:49.017817: train_loss -0.8927
2025-11-30 01:51:49.024769: val_loss -0.5832
2025-11-30 01:51:49.029480: Pseudo dice [np.float32(0.7702), np.float32(0.8712)]
2025-11-30 01:51:49.033651: Epoch time: 199.33 s
2025-11-30 01:51:49.896732: 
2025-11-30 01:51:49.909928: Epoch 101
2025-11-30 01:51:49.914141: Current learning rate: 0.00909
2025-11-30 01:55:09.155434: train_loss -0.9023
2025-11-30 01:55:09.163844: val_loss -0.6341
2025-11-30 01:55:09.167763: Pseudo dice [np.float32(0.8035), np.float32(0.8915)]
2025-11-30 01:55:09.171868: Epoch time: 199.26 s
2025-11-30 01:55:10.021982: 
2025-11-30 01:55:10.028708: Epoch 102
2025-11-30 01:55:10.036740: Current learning rate: 0.00908
2025-11-30 01:58:29.243558: train_loss -0.9033
2025-11-30 01:58:29.251213: val_loss -0.6972
2025-11-30 01:58:29.260547: Pseudo dice [np.float32(0.8659), np.float32(0.9049)]
2025-11-30 01:58:29.265256: Epoch time: 199.22 s
2025-11-30 01:58:30.117895: 
2025-11-30 01:58:30.125014: Epoch 103
2025-11-30 01:58:30.128651: Current learning rate: 0.00907
2025-11-30 02:01:49.252965: train_loss -0.907
2025-11-30 02:01:49.258253: val_loss -0.6785
2025-11-30 02:01:49.262264: Pseudo dice [np.float32(0.8311), np.float32(0.9015)]
2025-11-30 02:01:49.264841: Epoch time: 199.14 s
2025-11-30 02:01:50.101308: 
2025-11-30 02:01:50.112991: Epoch 104
2025-11-30 02:01:50.117561: Current learning rate: 0.00906
2025-11-30 02:05:09.286089: train_loss -0.9052
2025-11-30 02:05:09.293452: val_loss -0.6019
2025-11-30 02:05:09.297605: Pseudo dice [np.float32(0.7891), np.float32(0.8886)]
2025-11-30 02:05:09.308286: Epoch time: 199.19 s
2025-11-30 02:05:10.179773: 
2025-11-30 02:05:10.186764: Epoch 105
2025-11-30 02:05:10.192604: Current learning rate: 0.00905
2025-11-30 02:08:29.428483: train_loss -0.9092
2025-11-30 02:08:29.438595: val_loss -0.7963
2025-11-30 02:08:29.443065: Pseudo dice [np.float32(0.9134), np.float32(0.9232)]
2025-11-30 02:08:29.448806: Epoch time: 199.25 s
2025-11-30 02:08:30.309726: 
2025-11-30 02:08:30.313708: Epoch 106
2025-11-30 02:08:30.318336: Current learning rate: 0.00904
2025-11-30 02:11:49.520095: train_loss -0.9077
2025-11-30 02:11:49.526181: val_loss -0.7584
2025-11-30 02:11:49.529481: Pseudo dice [np.float32(0.9109), np.float32(0.922)]
2025-11-30 02:11:49.534438: Epoch time: 199.21 s
2025-11-30 02:11:50.918536: 
2025-11-30 02:11:50.924023: Epoch 107
2025-11-30 02:11:50.928238: Current learning rate: 0.00903
2025-11-30 02:15:10.137508: train_loss -0.9187
2025-11-30 02:15:10.142502: val_loss -0.7042
2025-11-30 02:15:10.145571: Pseudo dice [np.float32(0.8682), np.float32(0.9135)]
2025-11-30 02:15:10.149899: Epoch time: 199.22 s
2025-11-30 02:15:10.990598: 
2025-11-30 02:15:10.994292: Epoch 108
2025-11-30 02:15:10.997444: Current learning rate: 0.00902
2025-11-30 02:18:30.261243: train_loss -0.9219
2025-11-30 02:18:30.268349: val_loss -0.6572
2025-11-30 02:18:30.272223: Pseudo dice [np.float32(0.8336), np.float32(0.9025)]
2025-11-30 02:18:30.276245: Epoch time: 199.27 s
2025-11-30 02:18:31.120792: 
2025-11-30 02:18:31.127943: Epoch 109
2025-11-30 02:18:31.133818: Current learning rate: 0.00901
2025-11-30 02:21:50.393808: train_loss -0.9236
2025-11-30 02:21:50.400093: val_loss -0.6975
2025-11-30 02:21:50.414268: Pseudo dice [np.float32(0.8579), np.float32(0.9055)]
2025-11-30 02:21:50.418574: Epoch time: 199.27 s
2025-11-30 02:21:51.447713: 
2025-11-30 02:21:51.456877: Epoch 110
2025-11-30 02:21:51.482498: Current learning rate: 0.009
2025-11-30 02:25:10.698020: train_loss -0.9208
2025-11-30 02:25:10.713073: val_loss -0.7015
2025-11-30 02:25:10.718946: Pseudo dice [np.float32(0.871), np.float32(0.9093)]
2025-11-30 02:25:10.723209: Epoch time: 199.25 s
2025-11-30 02:25:11.582051: 
2025-11-30 02:25:11.586733: Epoch 111
2025-11-30 02:25:11.593685: Current learning rate: 0.009
2025-11-30 02:28:30.773550: train_loss -0.9203
2025-11-30 02:28:30.783573: val_loss -0.736
2025-11-30 02:28:30.788450: Pseudo dice [np.float32(0.8853), np.float32(0.9203)]
2025-11-30 02:28:30.793108: Epoch time: 199.19 s
2025-11-30 02:28:31.668315: 
2025-11-30 02:28:31.673346: Epoch 112
2025-11-30 02:28:31.678612: Current learning rate: 0.00899
2025-11-30 02:31:50.925483: train_loss -0.9245
2025-11-30 02:31:50.933319: val_loss -0.7267
2025-11-30 02:31:50.939834: Pseudo dice [np.float32(0.8711), np.float32(0.9143)]
2025-11-30 02:31:50.944149: Epoch time: 199.26 s
2025-11-30 02:31:51.796814: 
2025-11-30 02:31:51.809550: Epoch 113
2025-11-30 02:31:51.813983: Current learning rate: 0.00898
2025-11-30 02:35:11.063783: train_loss -0.9247
2025-11-30 02:35:11.071080: val_loss -0.707
2025-11-30 02:35:11.076541: Pseudo dice [np.float32(0.8748), np.float32(0.9111)]
2025-11-30 02:35:11.080153: Epoch time: 199.27 s
2025-11-30 02:35:11.928682: 
2025-11-30 02:35:11.937639: Epoch 114
2025-11-30 02:35:11.944264: Current learning rate: 0.00897
2025-11-30 02:38:31.160573: train_loss -0.9272
2025-11-30 02:38:31.167541: val_loss -0.7584
2025-11-30 02:38:31.170196: Pseudo dice [np.float32(0.91), np.float32(0.9251)]
2025-11-30 02:38:31.173330: Epoch time: 199.23 s
2025-11-30 02:38:31.176490: Yayy! New best EMA pseudo Dice: 0.883400022983551
2025-11-30 02:38:33.065563: 
2025-11-30 02:38:33.069017: Epoch 115
2025-11-30 02:38:33.071612: Current learning rate: 0.00896
2025-11-30 02:41:52.385770: train_loss -0.9265
2025-11-30 02:41:52.393130: val_loss -0.6464
2025-11-30 02:41:52.398099: Pseudo dice [np.float32(0.8222), np.float32(0.9016)]
2025-11-30 02:41:52.411291: Epoch time: 199.32 s
2025-11-30 02:41:53.743704: 
2025-11-30 02:41:53.752309: Epoch 116
2025-11-30 02:41:53.758024: Current learning rate: 0.00895
2025-11-30 02:45:13.074753: train_loss -0.9266
2025-11-30 02:45:13.082317: val_loss -0.7103
2025-11-30 02:45:13.086818: Pseudo dice [np.float32(0.8799), np.float32(0.9164)]
2025-11-30 02:45:13.090724: Epoch time: 199.33 s
2025-11-30 02:45:13.957297: 
2025-11-30 02:45:13.965020: Epoch 117
2025-11-30 02:45:13.969407: Current learning rate: 0.00894
2025-11-30 02:48:33.213304: train_loss -0.9267
2025-11-30 02:48:33.220803: val_loss -0.7627
2025-11-30 02:48:33.227730: Pseudo dice [np.float32(0.9012), np.float32(0.9204)]
2025-11-30 02:48:33.231717: Epoch time: 199.26 s
2025-11-30 02:48:33.235534: Yayy! New best EMA pseudo Dice: 0.885699987411499
2025-11-30 02:48:35.101713: 
2025-11-30 02:48:35.112780: Epoch 118
2025-11-30 02:48:35.117020: Current learning rate: 0.00893
2025-11-30 02:51:54.357064: train_loss -0.9231
2025-11-30 02:51:54.362067: val_loss -0.7213
2025-11-30 02:51:54.365014: Pseudo dice [np.float32(0.8774), np.float32(0.9148)]
2025-11-30 02:51:54.368669: Epoch time: 199.26 s
2025-11-30 02:51:54.371578: Yayy! New best EMA pseudo Dice: 0.8867999911308289
2025-11-30 02:51:56.567077: 
2025-11-30 02:51:56.571545: Epoch 119
2025-11-30 02:51:56.576827: Current learning rate: 0.00892
2025-11-30 02:55:15.901228: train_loss -0.9284
2025-11-30 02:55:15.913279: val_loss -0.7511
2025-11-30 02:55:15.915571: Pseudo dice [np.float32(0.9032), np.float32(0.9218)]
2025-11-30 02:55:15.919030: Epoch time: 199.34 s
2025-11-30 02:55:15.921592: Yayy! New best EMA pseudo Dice: 0.8892999887466431
2025-11-30 02:55:18.084056: 
2025-11-30 02:55:18.087887: Epoch 120
2025-11-30 02:55:18.090370: Current learning rate: 0.00891
2025-11-30 02:58:37.375670: train_loss -0.9273
2025-11-30 02:58:37.382099: val_loss -0.7256
2025-11-30 02:58:37.387263: Pseudo dice [np.float32(0.8825), np.float32(0.9155)]
2025-11-30 02:58:37.391541: Epoch time: 199.29 s
2025-11-30 02:58:37.397359: Yayy! New best EMA pseudo Dice: 0.8902999758720398
2025-11-30 02:58:39.550017: 
2025-11-30 02:58:39.554469: Epoch 121
2025-11-30 02:58:39.557916: Current learning rate: 0.0089
2025-11-30 03:01:58.827809: train_loss -0.9213
2025-11-30 03:01:58.833944: val_loss -0.6013
2025-11-30 03:01:58.838246: Pseudo dice [np.float32(0.8128), np.float32(0.9025)]
2025-11-30 03:01:58.842407: Epoch time: 199.28 s
2025-11-30 03:01:59.721566: 
2025-11-30 03:01:59.726532: Epoch 122
2025-11-30 03:01:59.730709: Current learning rate: 0.00889
2025-11-30 03:05:19.010301: train_loss -0.9266
2025-11-30 03:05:19.018315: val_loss -0.6514
2025-11-30 03:05:19.042237: Pseudo dice [np.float32(0.8707), np.float32(0.9193)]
2025-11-30 03:05:19.046803: Epoch time: 199.29 s
2025-11-30 03:05:19.944288: 
2025-11-30 03:05:19.952676: Epoch 123
2025-11-30 03:05:19.957771: Current learning rate: 0.00889
2025-11-30 03:08:39.337589: train_loss -0.9296
2025-11-30 03:08:39.344834: val_loss -0.6426
2025-11-30 03:08:39.352009: Pseudo dice [np.float32(0.8293), np.float32(0.906)]
2025-11-30 03:08:39.356412: Epoch time: 199.39 s
2025-11-30 03:08:40.217877: 
2025-11-30 03:08:40.221831: Epoch 124
2025-11-30 03:08:40.225065: Current learning rate: 0.00888
2025-11-30 03:11:59.666636: train_loss -0.9281
2025-11-30 03:11:59.679481: val_loss -0.5985
2025-11-30 03:11:59.682523: Pseudo dice [np.float32(0.7817), np.float32(0.8971)]
2025-11-30 03:11:59.685227: Epoch time: 199.45 s
2025-11-30 03:12:00.660835: 
2025-11-30 03:12:00.664904: Epoch 125
2025-11-30 03:12:00.669288: Current learning rate: 0.00887
2025-11-30 03:15:19.996947: train_loss -0.9083
2025-11-30 03:15:20.015953: val_loss -0.675
2025-11-30 03:15:20.021218: Pseudo dice [np.float32(0.8559), np.float32(0.8973)]
2025-11-30 03:15:20.025235: Epoch time: 199.34 s
2025-11-30 03:15:20.890561: 
2025-11-30 03:15:20.895253: Epoch 126
2025-11-30 03:15:20.900556: Current learning rate: 0.00886
2025-11-30 03:18:40.323392: train_loss -0.8838
2025-11-30 03:18:40.331317: val_loss -0.6087
2025-11-30 03:18:40.336421: Pseudo dice [np.float32(0.8185), np.float32(0.8929)]
2025-11-30 03:18:40.343277: Epoch time: 199.43 s
2025-11-30 03:18:41.196152: 
2025-11-30 03:18:41.200457: Epoch 127
2025-11-30 03:18:41.212730: Current learning rate: 0.00885
2025-11-30 03:22:00.501000: train_loss -0.9007
2025-11-30 03:22:00.513733: val_loss -0.7528
2025-11-30 03:22:00.517367: Pseudo dice [np.float32(0.9022), np.float32(0.9183)]
2025-11-30 03:22:00.520398: Epoch time: 199.31 s
2025-11-30 03:22:01.389969: 
2025-11-30 03:22:01.396741: Epoch 128
2025-11-30 03:22:01.400465: Current learning rate: 0.00884
2025-11-30 03:25:20.721309: train_loss -0.9027
2025-11-30 03:25:20.727908: val_loss -0.5539
2025-11-30 03:25:20.734525: Pseudo dice [np.float32(0.7433), np.float32(0.8862)]
2025-11-30 03:25:20.739347: Epoch time: 199.33 s
2025-11-30 03:25:21.611765: 
2025-11-30 03:25:21.616346: Epoch 129
2025-11-30 03:25:21.620506: Current learning rate: 0.00883
2025-11-30 03:28:40.905496: train_loss -0.8942
2025-11-30 03:28:40.913915: val_loss -0.7101
2025-11-30 03:28:40.918852: Pseudo dice [np.float32(0.8731), np.float32(0.9096)]
2025-11-30 03:28:40.942467: Epoch time: 199.29 s
2025-11-30 03:28:41.813286: 
2025-11-30 03:28:41.819910: Epoch 130
2025-11-30 03:28:41.824276: Current learning rate: 0.00882
2025-11-30 03:32:01.053744: train_loss -0.893
2025-11-30 03:32:01.059912: val_loss -0.637
2025-11-30 03:32:01.063576: Pseudo dice [np.float32(0.8119), np.float32(0.8971)]
2025-11-30 03:32:01.066925: Epoch time: 199.24 s
2025-11-30 03:32:02.141741: 
2025-11-30 03:32:02.155456: Epoch 131
2025-11-30 03:32:02.160257: Current learning rate: 0.00881
2025-11-30 03:35:21.488404: train_loss -0.8886
2025-11-30 03:35:21.498076: val_loss -0.6385
2025-11-30 03:35:21.511729: Pseudo dice [np.float32(0.8532), np.float32(0.8875)]
2025-11-30 03:35:21.517663: Epoch time: 199.35 s
2025-11-30 03:35:22.439686: 
2025-11-30 03:35:22.444639: Epoch 132
2025-11-30 03:35:22.451092: Current learning rate: 0.0088
2025-11-30 03:38:41.726996: train_loss -0.8964
2025-11-30 03:38:41.737291: val_loss -0.6489
2025-11-30 03:38:41.741846: Pseudo dice [np.float32(0.8045), np.float32(0.8926)]
2025-11-30 03:38:41.746881: Epoch time: 199.29 s
2025-11-30 03:38:43.212559: 
2025-11-30 03:38:43.219142: Epoch 133
2025-11-30 03:38:43.221905: Current learning rate: 0.00879
2025-11-30 03:42:02.635041: train_loss -0.8957
2025-11-30 03:42:02.643321: val_loss -0.6681
2025-11-30 03:42:02.651685: Pseudo dice [np.float32(0.8683), np.float32(0.8972)]
2025-11-30 03:42:02.656546: Epoch time: 199.42 s
2025-11-30 03:42:03.641516: 
2025-11-30 03:42:03.652098: Epoch 134
2025-11-30 03:42:03.660493: Current learning rate: 0.00879
