
############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-11-29 07:24:08.152048: Using torch.compile...
2025-11-29 07:24:12.481796: do_dummy_2d_data_aug: False
2025-11-29 07:24:12.491860: Using splits from existing split file: /workspace/onedrive/ccmr/ccmr/data/nnunet/nnUNet_preprocessed/Dataset940_HVSMR_L40/splits_final.json
2025-11-29 07:24:12.529205: The split file contains 5 splits.
2025-11-29 07:24:12.541208: Desired fold for training: 0
2025-11-29 07:24:12.545545: This split has 32 training and 8 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 160, 112], 'median_image_size_in_voxels': [125.0, 149.0, 104.5], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset940_HVSMR_L40', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [125, 149, 104], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2.1471028327941895, 'mean': 0.7456470131874084, 'median': 0.7674959301948547, 'min': -0.03358686342835426, 'percentile_00_5': 0.26383838057518005, 'percentile_99_5': 1.1330369710922241, 'std': 0.16175414621829987}}} 

2025-11-29 07:24:18.692065: Unable to plot network architecture: nnUNet_compile is enabled!
2025-11-29 07:24:18.791078: 
2025-11-29 07:24:18.795276: Epoch 0
2025-11-29 07:24:18.798785: Current learning rate: 0.01
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
/workspace/onedrive/ccmr/ccmr/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  output = output[crop_slices].contiguous()
2025-11-29 07:31:51.478728: train_loss -0.1091
2025-11-29 07:31:51.483222: val_loss -0.2913
2025-11-29 07:31:51.486181: Pseudo dice [np.float32(0.0), np.float32(0.8012)]
2025-11-29 07:31:51.489172: Epoch time: 452.69 s
2025-11-29 07:31:51.492065: Yayy! New best EMA pseudo Dice: 0.40059998631477356
2025-11-29 07:31:53.153556: 
2025-11-29 07:31:53.156992: Epoch 1
2025-11-29 07:31:53.160005: Current learning rate: 0.00999
2025-11-29 07:39:08.979365: train_loss -0.3855
2025-11-29 07:39:08.985145: val_loss -0.4937
2025-11-29 07:39:08.987928: Pseudo dice [np.float32(0.6157), np.float32(0.8001)]
2025-11-29 07:39:08.990978: Epoch time: 435.83 s
2025-11-29 07:39:08.993587: Yayy! New best EMA pseudo Dice: 0.43130001425743103
2025-11-29 07:39:10.578472: 
2025-11-29 07:39:10.581643: Epoch 2
2025-11-29 07:39:10.584393: Current learning rate: 0.00998
2025-11-29 07:46:26.511126: train_loss -0.5238
2025-11-29 07:46:26.516789: val_loss -0.5742
2025-11-29 07:46:26.519921: Pseudo dice [np.float32(0.7116), np.float32(0.8287)]
2025-11-29 07:46:26.522779: Epoch time: 435.93 s
2025-11-29 07:46:26.525723: Yayy! New best EMA pseudo Dice: 0.4652000069618225
2025-11-29 07:46:28.446509: 
2025-11-29 07:46:28.450006: Epoch 3
2025-11-29 07:46:28.454131: Current learning rate: 0.00997
2025-11-29 07:53:44.020240: train_loss -0.596
2025-11-29 07:53:44.024275: val_loss -0.5299
2025-11-29 07:53:44.027071: Pseudo dice [np.float32(0.6823), np.float32(0.826)]
2025-11-29 07:53:44.029949: Epoch time: 435.57 s
2025-11-29 07:53:44.031964: Yayy! New best EMA pseudo Dice: 0.49410000443458557
2025-11-29 07:53:46.076791: 
2025-11-29 07:53:46.080464: Epoch 4
2025-11-29 07:53:46.083223: Current learning rate: 0.00996
2025-11-29 08:01:01.428220: train_loss -0.655
2025-11-29 08:01:01.432687: val_loss -0.5364
2025-11-29 08:01:01.435160: Pseudo dice [np.float32(0.7146), np.float32(0.843)]
2025-11-29 08:01:01.437491: Epoch time: 435.35 s
2025-11-29 08:01:01.439932: Yayy! New best EMA pseudo Dice: 0.522599995136261
2025-11-29 08:01:03.422851: 
2025-11-29 08:01:03.426131: Epoch 5
2025-11-29 08:01:03.429257: Current learning rate: 0.00995
2025-11-29 08:08:18.833712: train_loss -0.6656
2025-11-29 08:08:18.838942: val_loss -0.5723
2025-11-29 08:08:18.841930: Pseudo dice [np.float32(0.7553), np.float32(0.8439)]
2025-11-29 08:08:18.845118: Epoch time: 435.41 s
2025-11-29 08:08:18.847724: Yayy! New best EMA pseudo Dice: 0.5503000020980835
2025-11-29 08:08:20.653878: 
2025-11-29 08:08:20.657308: Epoch 6
2025-11-29 08:08:20.660366: Current learning rate: 0.00995
2025-11-29 08:15:36.312642: train_loss -0.7167
2025-11-29 08:15:36.317545: val_loss -0.5795
2025-11-29 08:15:36.320194: Pseudo dice [np.float32(0.7298), np.float32(0.8576)]
2025-11-29 08:15:36.323082: Epoch time: 435.66 s
2025-11-29 08:15:36.325962: Yayy! New best EMA pseudo Dice: 0.5745999813079834
2025-11-29 08:15:38.408887: 
2025-11-29 08:15:38.414119: Epoch 7
2025-11-29 08:15:38.418096: Current learning rate: 0.00994
2025-11-29 08:22:53.969538: train_loss -0.7167
2025-11-29 08:22:53.974267: val_loss -0.572
2025-11-29 08:22:53.977729: Pseudo dice [np.float32(0.6905), np.float32(0.8457)]
2025-11-29 08:22:54.008237: Epoch time: 435.56 s
2025-11-29 08:22:54.012043: Yayy! New best EMA pseudo Dice: 0.593999981880188
2025-11-29 08:22:55.913265: 
2025-11-29 08:22:55.916472: Epoch 8
2025-11-29 08:22:55.919420: Current learning rate: 0.00993
2025-11-29 08:30:11.423857: train_loss -0.7339
2025-11-29 08:30:11.431534: val_loss -0.5629
2025-11-29 08:30:11.434314: Pseudo dice [np.float32(0.7156), np.float32(0.8455)]
2025-11-29 08:30:11.437698: Epoch time: 435.51 s
2025-11-29 08:30:11.440306: Yayy! New best EMA pseudo Dice: 0.6126000285148621
2025-11-29 08:30:13.324509: 
2025-11-29 08:30:13.330309: Epoch 9
2025-11-29 08:30:13.335079: Current learning rate: 0.00992
2025-11-29 08:37:28.618433: train_loss -0.7362
2025-11-29 08:37:28.623932: val_loss -0.556
2025-11-29 08:37:28.627200: Pseudo dice [np.float32(0.6687), np.float32(0.8314)]
2025-11-29 08:37:28.629912: Epoch time: 435.3 s
2025-11-29 08:37:28.632691: Yayy! New best EMA pseudo Dice: 0.6263999938964844
2025-11-29 08:37:30.753479: 
2025-11-29 08:37:30.758014: Epoch 10
2025-11-29 08:37:30.761436: Current learning rate: 0.00991
2025-11-29 08:44:46.487783: train_loss -0.7468
2025-11-29 08:44:46.492735: val_loss -0.5967
2025-11-29 08:44:46.495749: Pseudo dice [np.float32(0.7546), np.float32(0.8617)]
2025-11-29 08:44:46.498121: Epoch time: 435.74 s
2025-11-29 08:44:46.509114: Yayy! New best EMA pseudo Dice: 0.6445000171661377
2025-11-29 08:44:48.349506: 
2025-11-29 08:44:48.352565: Epoch 11
2025-11-29 08:44:48.355492: Current learning rate: 0.0099
2025-11-29 08:52:05.015127: train_loss -0.7581
2025-11-29 08:52:05.020941: val_loss -0.6571
2025-11-29 08:52:05.023690: Pseudo dice [np.float32(0.8116), np.float32(0.8684)]
2025-11-29 08:52:05.027139: Epoch time: 436.67 s
2025-11-29 08:52:05.029766: Yayy! New best EMA pseudo Dice: 0.6640999913215637
2025-11-29 08:52:06.950498: 
2025-11-29 08:52:06.953607: Epoch 12
2025-11-29 08:52:06.956784: Current learning rate: 0.00989
2025-11-29 08:59:22.303551: train_loss -0.7686
2025-11-29 08:59:22.311224: val_loss -0.546
2025-11-29 08:59:22.314172: Pseudo dice [np.float32(0.7145), np.float32(0.8741)]
2025-11-29 08:59:22.317529: Epoch time: 435.35 s
2025-11-29 08:59:22.319808: Yayy! New best EMA pseudo Dice: 0.6771000027656555
2025-11-29 08:59:24.266506: 
2025-11-29 08:59:24.269860: Epoch 13
2025-11-29 08:59:24.272632: Current learning rate: 0.00988
2025-11-29 09:06:39.804206: train_loss -0.7656
2025-11-29 09:06:39.812452: val_loss -0.5335
2025-11-29 09:06:39.815161: Pseudo dice [np.float32(0.6459), np.float32(0.8603)]
2025-11-29 09:06:39.817582: Epoch time: 435.54 s
2025-11-29 09:06:39.820158: Yayy! New best EMA pseudo Dice: 0.6847000122070312
2025-11-29 09:06:41.737468: 
2025-11-29 09:06:41.741033: Epoch 14
2025-11-29 09:06:41.744206: Current learning rate: 0.00987
2025-11-29 09:13:57.338585: train_loss -0.7833
2025-11-29 09:13:57.344967: val_loss -0.6245
2025-11-29 09:13:57.347411: Pseudo dice [np.float32(0.7468), np.float32(0.8627)]
2025-11-29 09:13:57.349959: Epoch time: 435.6 s
2025-11-29 09:13:57.353384: Yayy! New best EMA pseudo Dice: 0.6966999769210815
2025-11-29 09:13:59.188570: 
2025-11-29 09:13:59.192011: Epoch 15
2025-11-29 09:13:59.194809: Current learning rate: 0.00986
2025-11-29 09:21:14.929579: train_loss -0.7771
2025-11-29 09:21:14.935408: val_loss -0.5225
2025-11-29 09:21:14.937722: Pseudo dice [np.float32(0.6212), np.float32(0.8379)]
2025-11-29 09:21:14.940307: Epoch time: 435.74 s
2025-11-29 09:21:14.942679: Yayy! New best EMA pseudo Dice: 0.699999988079071
2025-11-29 09:21:16.918192: 
2025-11-29 09:21:16.924045: Epoch 16
2025-11-29 09:21:16.927282: Current learning rate: 0.00986
2025-11-29 09:28:33.615129: train_loss -0.7852
2025-11-29 09:28:33.621624: val_loss -0.6576
2025-11-29 09:28:33.624004: Pseudo dice [np.float32(0.7879), np.float32(0.8747)]
2025-11-29 09:28:33.627194: Epoch time: 436.7 s
2025-11-29 09:28:33.629527: Yayy! New best EMA pseudo Dice: 0.713100016117096
2025-11-29 09:28:35.461434: 
2025-11-29 09:28:35.464331: Epoch 17
2025-11-29 09:28:35.467192: Current learning rate: 0.00985
2025-11-29 09:35:50.996181: train_loss -0.8013
2025-11-29 09:35:51.007819: val_loss -0.662
2025-11-29 09:35:51.011943: Pseudo dice [np.float32(0.7895), np.float32(0.8748)]
2025-11-29 09:35:51.015989: Epoch time: 435.54 s
2025-11-29 09:35:51.019677: Yayy! New best EMA pseudo Dice: 0.7250000238418579
2025-11-29 09:35:53.060271: 
2025-11-29 09:35:53.063988: Epoch 18
2025-11-29 09:35:53.067390: Current learning rate: 0.00984
2025-11-29 09:43:08.370226: train_loss -0.8066
2025-11-29 09:43:08.377913: val_loss -0.5689
2025-11-29 09:43:08.383443: Pseudo dice [np.float32(0.6938), np.float32(0.8527)]
2025-11-29 09:43:08.387043: Epoch time: 435.31 s
2025-11-29 09:43:08.392106: Yayy! New best EMA pseudo Dice: 0.7297999858856201
2025-11-29 09:43:10.528998: 
2025-11-29 09:43:10.534801: Epoch 19
2025-11-29 09:43:10.539773: Current learning rate: 0.00983
2025-11-29 09:50:25.662452: train_loss -0.8091
2025-11-29 09:50:25.667372: val_loss -0.4699
2025-11-29 09:50:25.670454: Pseudo dice [np.float32(0.6214), np.float32(0.8154)]
2025-11-29 09:50:25.672942: Epoch time: 435.13 s
2025-11-29 09:50:26.567870: 
2025-11-29 09:50:26.571742: Epoch 20
2025-11-29 09:50:26.574721: Current learning rate: 0.00982
2025-11-29 09:57:42.355301: train_loss -0.8054
2025-11-29 09:57:42.361188: val_loss -0.4746
2025-11-29 09:57:42.363898: Pseudo dice [np.float32(0.6114), np.float32(0.8186)]
2025-11-29 09:57:42.367602: Epoch time: 435.79 s
2025-11-29 09:57:43.341522: 
2025-11-29 09:57:43.347866: Epoch 21
2025-11-29 09:57:43.352510: Current learning rate: 0.00981
2025-11-29 10:04:57.861038: train_loss -0.8102
2025-11-29 10:04:57.865614: val_loss -0.6524
2025-11-29 10:04:57.868255: Pseudo dice [np.float32(0.7821), np.float32(0.8729)]
2025-11-29 10:04:57.871104: Epoch time: 434.52 s
2025-11-29 10:04:57.874128: Yayy! New best EMA pseudo Dice: 0.7373999953269958
2025-11-29 10:04:59.647217: 
2025-11-29 10:04:59.650049: Epoch 22
2025-11-29 10:04:59.653428: Current learning rate: 0.0098
2025-11-29 10:12:15.278111: train_loss -0.8323
2025-11-29 10:12:15.285737: val_loss -0.6776
2025-11-29 10:12:15.288445: Pseudo dice [np.float32(0.8436), np.float32(0.9003)]
2025-11-29 10:12:15.291241: Epoch time: 435.63 s
2025-11-29 10:12:15.293651: Yayy! New best EMA pseudo Dice: 0.7508000135421753
2025-11-29 10:12:17.010074: 
2025-11-29 10:12:17.013404: Epoch 23
2025-11-29 10:12:17.016428: Current learning rate: 0.00979
2025-11-29 10:19:32.441139: train_loss -0.8334
2025-11-29 10:19:32.445812: val_loss -0.647
2025-11-29 10:19:32.449308: Pseudo dice [np.float32(0.8035), np.float32(0.8751)]
2025-11-29 10:19:32.452225: Epoch time: 435.43 s
2025-11-29 10:19:32.454804: Yayy! New best EMA pseudo Dice: 0.7597000002861023
2025-11-29 10:19:34.523626: 
2025-11-29 10:19:34.529776: Epoch 24
2025-11-29 10:19:34.534682: Current learning rate: 0.00978
2025-11-29 10:26:49.935366: train_loss -0.8329
2025-11-29 10:26:49.940970: val_loss -0.655
2025-11-29 10:26:49.943416: Pseudo dice [np.float32(0.7566), np.float32(0.8717)]
2025-11-29 10:26:49.946079: Epoch time: 435.41 s
2025-11-29 10:26:49.949125: Yayy! New best EMA pseudo Dice: 0.7651000022888184
2025-11-29 10:26:51.759197: 
2025-11-29 10:26:51.764045: Epoch 25
2025-11-29 10:26:51.768378: Current learning rate: 0.00977
2025-11-29 10:34:07.056105: train_loss -0.8396
2025-11-29 10:34:07.060652: val_loss -0.6421
2025-11-29 10:34:07.063450: Pseudo dice [np.float32(0.7798), np.float32(0.8849)]
2025-11-29 10:34:07.066020: Epoch time: 435.3 s
2025-11-29 10:34:07.068895: Yayy! New best EMA pseudo Dice: 0.7717999815940857
2025-11-29 10:34:08.784587: 
2025-11-29 10:34:08.787759: Epoch 26
2025-11-29 10:34:08.790868: Current learning rate: 0.00977
2025-11-29 10:41:24.000971: train_loss -0.8492
2025-11-29 10:41:24.014119: val_loss -0.6301
2025-11-29 10:41:24.017040: Pseudo dice [np.float32(0.755), np.float32(0.8645)]
2025-11-29 10:41:24.020038: Epoch time: 435.22 s
2025-11-29 10:41:24.023086: Yayy! New best EMA pseudo Dice: 0.775600016117096
2025-11-29 10:41:25.934964: 
2025-11-29 10:41:25.937935: Epoch 27
2025-11-29 10:41:25.940953: Current learning rate: 0.00976
2025-11-29 10:48:42.364327: train_loss -0.8524
2025-11-29 10:48:42.371760: val_loss -0.5989
2025-11-29 10:48:42.378613: Pseudo dice [np.float32(0.7215), np.float32(0.869)]
2025-11-29 10:48:42.383878: Epoch time: 436.43 s
2025-11-29 10:48:42.388665: Yayy! New best EMA pseudo Dice: 0.7775999903678894
2025-11-29 10:48:44.444762: 
2025-11-29 10:48:44.448898: Epoch 28
2025-11-29 10:48:44.452225: Current learning rate: 0.00975
2025-11-29 10:56:00.975623: train_loss -0.8477
2025-11-29 10:56:00.980907: val_loss -0.6531
2025-11-29 10:56:00.983781: Pseudo dice [np.float32(0.7838), np.float32(0.8843)]
2025-11-29 10:56:00.986381: Epoch time: 436.53 s
2025-11-29 10:56:00.988816: Yayy! New best EMA pseudo Dice: 0.7832000255584717
2025-11-29 10:56:02.952328: 
2025-11-29 10:56:02.957444: Epoch 29
2025-11-29 10:56:02.962241: Current learning rate: 0.00974
2025-11-29 11:03:18.427248: train_loss -0.8561
2025-11-29 11:03:18.431783: val_loss -0.5012
2025-11-29 11:03:18.434552: Pseudo dice [np.float32(0.6249), np.float32(0.8618)]
2025-11-29 11:03:18.437412: Epoch time: 435.48 s
2025-11-29 11:03:19.294188: 
2025-11-29 11:03:19.297103: Epoch 30
2025-11-29 11:03:19.300114: Current learning rate: 0.00973
2025-11-29 11:10:34.166755: train_loss -0.8416
2025-11-29 11:10:34.171697: val_loss -0.6017
2025-11-29 11:10:34.173967: Pseudo dice [np.float32(0.7534), np.float32(0.8768)]
2025-11-29 11:10:34.196005: Epoch time: 434.87 s
2025-11-29 11:10:35.088050: 
2025-11-29 11:10:35.090814: Epoch 31
2025-11-29 11:10:35.093189: Current learning rate: 0.00972
2025-11-29 11:17:52.031847: train_loss -0.8582
2025-11-29 11:17:52.037165: val_loss -0.6737
2025-11-29 11:17:52.039927: Pseudo dice [np.float32(0.8426), np.float32(0.9002)]
2025-11-29 11:17:52.043180: Epoch time: 436.94 s
2025-11-29 11:17:52.046527: Yayy! New best EMA pseudo Dice: 0.791700005531311
2025-11-29 11:17:54.091522: 
2025-11-29 11:17:54.094558: Epoch 32
2025-11-29 11:17:54.098679: Current learning rate: 0.00971
2025-11-29 11:25:10.007592: train_loss -0.8547
2025-11-29 11:25:10.013979: val_loss -0.5382
2025-11-29 11:25:10.017160: Pseudo dice [np.float32(0.6946), np.float32(0.8673)]
2025-11-29 11:25:10.020285: Epoch time: 435.92 s
2025-11-29 11:25:10.876720: 
2025-11-29 11:25:10.880031: Epoch 33
2025-11-29 11:25:10.883871: Current learning rate: 0.0097
2025-11-29 11:32:26.674986: train_loss -0.8439
2025-11-29 11:32:26.682378: val_loss -0.6217
2025-11-29 11:32:26.686397: Pseudo dice [np.float32(0.7818), np.float32(0.8827)]
2025-11-29 11:32:26.690195: Epoch time: 435.8 s
2025-11-29 11:32:26.694934: Yayy! New best EMA pseudo Dice: 0.7947999835014343
2025-11-29 11:32:29.334913: 
2025-11-29 11:32:29.339654: Epoch 34
2025-11-29 11:32:29.344365: Current learning rate: 0.00969
2025-11-29 11:39:44.983231: train_loss -0.8533
2025-11-29 11:39:44.988571: val_loss -0.6561
2025-11-29 11:39:44.991737: Pseudo dice [np.float32(0.7563), np.float32(0.8822)]
2025-11-29 11:39:44.994568: Epoch time: 435.65 s
2025-11-29 11:39:44.997473: Yayy! New best EMA pseudo Dice: 0.7972000241279602
2025-11-29 11:39:46.834020: 
2025-11-29 11:39:46.838717: Epoch 35
2025-11-29 11:39:46.843477: Current learning rate: 0.00968
2025-11-29 11:47:04.062353: train_loss -0.8585
2025-11-29 11:47:04.069724: val_loss -0.6241
2025-11-29 11:47:04.074875: Pseudo dice [np.float32(0.7375), np.float32(0.8809)]
2025-11-29 11:47:04.080851: Epoch time: 437.23 s
2025-11-29 11:47:04.086570: Yayy! New best EMA pseudo Dice: 0.7983999848365784
2025-11-29 11:47:06.169810: 
2025-11-29 11:47:06.174663: Epoch 36
2025-11-29 11:47:06.179114: Current learning rate: 0.00968
2025-11-29 11:54:23.480979: train_loss -0.8648
2025-11-29 11:54:23.486702: val_loss -0.642
2025-11-29 11:54:23.489950: Pseudo dice [np.float32(0.7912), np.float32(0.893)]
2025-11-29 11:54:23.493201: Epoch time: 437.31 s
2025-11-29 11:54:23.496093: Yayy! New best EMA pseudo Dice: 0.8027999997138977
2025-11-29 11:54:25.378869: 
2025-11-29 11:54:25.382142: Epoch 37
2025-11-29 11:54:25.384907: Current learning rate: 0.00967
2025-11-29 12:01:41.991384: train_loss -0.857
2025-11-29 12:01:41.999043: val_loss -0.6697
2025-11-29 12:01:42.011933: Pseudo dice [np.float32(0.8001), np.float32(0.8922)]
2025-11-29 12:01:42.016954: Epoch time: 436.61 s
2025-11-29 12:01:42.021466: Yayy! New best EMA pseudo Dice: 0.8070999979972839
2025-11-29 12:01:43.941937: 
2025-11-29 12:01:43.947564: Epoch 38
2025-11-29 12:01:43.951772: Current learning rate: 0.00966
2025-11-29 12:09:00.761295: train_loss -0.8558
2025-11-29 12:09:00.767394: val_loss -0.5667
2025-11-29 12:09:00.769794: Pseudo dice [np.float32(0.6735), np.float32(0.836)]
2025-11-29 12:09:00.772755: Epoch time: 436.82 s
2025-11-29 12:09:01.659923: 
2025-11-29 12:09:01.663642: Epoch 39
2025-11-29 12:09:01.668342: Current learning rate: 0.00965
2025-11-29 12:16:18.026036: train_loss -0.8527
2025-11-29 12:16:18.031146: val_loss -0.6191
2025-11-29 12:16:18.033791: Pseudo dice [np.float32(0.7399), np.float32(0.8587)]
2025-11-29 12:16:18.036684: Epoch time: 436.37 s
2025-11-29 12:16:18.929292: 
2025-11-29 12:16:18.932305: Epoch 40
2025-11-29 12:16:18.935254: Current learning rate: 0.00964
2025-11-29 12:23:35.609270: train_loss -0.8612
2025-11-29 12:23:35.615298: val_loss -0.6698
2025-11-29 12:23:35.619277: Pseudo dice [np.float32(0.8064), np.float32(0.8888)]
2025-11-29 12:23:35.623345: Epoch time: 436.68 s
2025-11-29 12:23:36.658225: 
2025-11-29 12:23:36.662077: Epoch 41
2025-11-29 12:23:36.665178: Current learning rate: 0.00963
2025-11-29 12:30:53.597205: train_loss -0.8688
2025-11-29 12:30:53.608612: val_loss -0.7051
2025-11-29 12:30:53.611759: Pseudo dice [np.float32(0.8061), np.float32(0.8925)]
2025-11-29 12:30:53.614340: Epoch time: 436.94 s
2025-11-29 12:30:53.617797: Yayy! New best EMA pseudo Dice: 0.8105000257492065
2025-11-29 12:30:55.345612: 
2025-11-29 12:30:55.348510: Epoch 42
2025-11-29 12:30:55.352261: Current learning rate: 0.00962
2025-11-29 12:38:12.362368: train_loss -0.868
2025-11-29 12:38:12.366424: val_loss -0.6506
2025-11-29 12:38:12.369041: Pseudo dice [np.float32(0.7466), np.float32(0.8702)]
2025-11-29 12:38:12.371396: Epoch time: 437.02 s
2025-11-29 12:38:13.231934: 
2025-11-29 12:38:13.234648: Epoch 43
2025-11-29 12:38:13.239899: Current learning rate: 0.00961
2025-11-29 12:45:30.181683: train_loss -0.861
2025-11-29 12:45:30.187926: val_loss -0.7069
2025-11-29 12:45:30.191269: Pseudo dice [np.float32(0.8329), np.float32(0.8899)]
2025-11-29 12:45:30.195824: Epoch time: 436.95 s
2025-11-29 12:45:30.200529: Yayy! New best EMA pseudo Dice: 0.8154000043869019
2025-11-29 12:45:32.069372: 
2025-11-29 12:45:32.072529: Epoch 44
2025-11-29 12:45:32.074984: Current learning rate: 0.0096
2025-11-29 12:52:49.287494: train_loss -0.8616
2025-11-29 12:52:49.294431: val_loss -0.6567
2025-11-29 12:52:49.297859: Pseudo dice [np.float32(0.7857), np.float32(0.8896)]
2025-11-29 12:52:49.300468: Epoch time: 437.22 s
2025-11-29 12:52:49.311972: Yayy! New best EMA pseudo Dice: 0.8176000118255615
2025-11-29 12:52:51.555346: 
2025-11-29 12:52:51.559579: Epoch 45
2025-11-29 12:52:51.564762: Current learning rate: 0.00959
2025-11-29 13:00:08.467392: train_loss -0.8662
2025-11-29 13:00:08.472090: val_loss -0.5919
2025-11-29 13:00:08.474799: Pseudo dice [np.float32(0.7507), np.float32(0.8743)]
2025-11-29 13:00:08.477453: Epoch time: 436.91 s
2025-11-29 13:00:09.500984: 
2025-11-29 13:00:09.513421: Epoch 46
2025-11-29 13:00:09.516211: Current learning rate: 0.00959
2025-11-29 13:07:25.744566: train_loss -0.8608
2025-11-29 13:07:25.750916: val_loss -0.5619
2025-11-29 13:07:25.754014: Pseudo dice [np.float32(0.6936), np.float32(0.8404)]
2025-11-29 13:07:25.757198: Epoch time: 436.24 s
2025-11-29 13:07:26.614688: 
2025-11-29 13:07:26.619515: Epoch 47
2025-11-29 13:07:26.631570: Current learning rate: 0.00958
2025-11-29 13:14:43.301935: train_loss -0.8521
2025-11-29 13:14:43.319579: val_loss -0.7049
2025-11-29 13:14:43.324401: Pseudo dice [np.float32(0.812), np.float32(0.8929)]
2025-11-29 13:14:43.329072: Epoch time: 436.69 s
2025-11-29 13:14:44.381852: 
2025-11-29 13:14:44.387678: Epoch 48
2025-11-29 13:14:44.393920: Current learning rate: 0.00957
2025-11-29 13:22:01.173760: train_loss -0.8681
2025-11-29 13:22:01.178230: val_loss -0.6388
2025-11-29 13:22:01.180954: Pseudo dice [np.float32(0.7559), np.float32(0.8735)]
2025-11-29 13:22:01.183464: Epoch time: 436.79 s
2025-11-29 13:22:02.217865: 
2025-11-29 13:22:02.222337: Epoch 49
2025-11-29 13:22:02.225370: Current learning rate: 0.00956
2025-11-29 13:29:19.166016: train_loss -0.8653
2025-11-29 13:29:19.170377: val_loss -0.6176
2025-11-29 13:29:19.173358: Pseudo dice [np.float32(0.762), np.float32(0.8799)]
2025-11-29 13:29:19.175932: Epoch time: 436.95 s
2025-11-29 13:29:21.119124: 
2025-11-29 13:29:21.121905: Epoch 50
2025-11-29 13:29:21.124615: Current learning rate: 0.00955
2025-11-29 13:36:37.136316: train_loss -0.8744
2025-11-29 13:36:37.140860: val_loss -0.6793
2025-11-29 13:36:37.143186: Pseudo dice [np.float32(0.7971), np.float32(0.8969)]
2025-11-29 13:36:37.145744: Epoch time: 436.02 s
2025-11-29 13:36:37.148034: Yayy! New best EMA pseudo Dice: 0.819599986076355
2025-11-29 13:36:39.030889: 
2025-11-29 13:36:39.033664: Epoch 51
2025-11-29 13:36:39.036525: Current learning rate: 0.00954
2025-11-29 13:43:54.724012: train_loss -0.8815
2025-11-29 13:43:54.744865: val_loss -0.6126
2025-11-29 13:43:54.748581: Pseudo dice [np.float32(0.7398), np.float32(0.8715)]
2025-11-29 13:43:54.751222: Epoch time: 435.69 s
2025-11-29 13:43:55.773632: 
2025-11-29 13:43:55.778389: Epoch 52
2025-11-29 13:43:55.781886: Current learning rate: 0.00953
2025-11-29 13:51:11.740894: train_loss -0.8813
2025-11-29 13:51:11.748291: val_loss -0.6171
2025-11-29 13:51:11.752908: Pseudo dice [np.float32(0.7504), np.float32(0.8766)]
2025-11-29 13:51:11.757501: Epoch time: 435.97 s
2025-11-29 13:51:12.761164: 
2025-11-29 13:51:12.767325: Epoch 53
2025-11-29 13:51:12.770996: Current learning rate: 0.00952
2025-11-29 13:58:30.004379: train_loss -0.8734
2025-11-29 13:58:30.013481: val_loss -0.6998
2025-11-29 13:58:30.016231: Pseudo dice [np.float32(0.8164), np.float32(0.901)]
2025-11-29 13:58:30.018993: Epoch time: 437.24 s
2025-11-29 13:58:30.022870: Yayy! New best EMA pseudo Dice: 0.8217999935150146
2025-11-29 13:58:31.814326: 
2025-11-29 13:58:31.817257: Epoch 54
2025-11-29 13:58:31.820269: Current learning rate: 0.00951
2025-11-29 14:05:48.634409: train_loss -0.8805
2025-11-29 14:05:48.641182: val_loss -0.6982
2025-11-29 14:05:48.643919: Pseudo dice [np.float32(0.8467), np.float32(0.9041)]
2025-11-29 14:05:48.647623: Epoch time: 436.82 s
2025-11-29 14:05:48.650912: Yayy! New best EMA pseudo Dice: 0.8271999955177307
2025-11-29 14:05:50.514862: 
2025-11-29 14:05:50.518338: Epoch 55
2025-11-29 14:05:50.520879: Current learning rate: 0.0095
2025-11-29 14:13:07.722090: train_loss -0.8896
2025-11-29 14:13:07.727163: val_loss -0.6376
2025-11-29 14:13:07.731353: Pseudo dice [np.float32(0.7802), np.float32(0.8933)]
2025-11-29 14:13:07.734330: Epoch time: 437.21 s
2025-11-29 14:13:07.739226: Yayy! New best EMA pseudo Dice: 0.8281000256538391
2025-11-29 14:13:09.861523: 
2025-11-29 14:13:09.865832: Epoch 56
2025-11-29 14:13:09.869524: Current learning rate: 0.00949
2025-11-29 14:20:27.015349: train_loss -0.8851
2025-11-29 14:20:27.019642: val_loss -0.622
2025-11-29 14:20:27.022773: Pseudo dice [np.float32(0.7349), np.float32(0.8577)]
2025-11-29 14:20:27.025246: Epoch time: 437.15 s
2025-11-29 14:20:28.026295: 
2025-11-29 14:20:28.030965: Epoch 57
2025-11-29 14:20:28.033783: Current learning rate: 0.00949
2025-11-29 14:27:45.372520: train_loss -0.8801
2025-11-29 14:27:45.378105: val_loss -0.6688
2025-11-29 14:27:45.380876: Pseudo dice [np.float32(0.8042), np.float32(0.8847)]
2025-11-29 14:27:45.383971: Epoch time: 437.35 s
2025-11-29 14:27:46.616033: 
2025-11-29 14:27:46.619757: Epoch 58
2025-11-29 14:27:46.634269: Current learning rate: 0.00948
2025-11-29 14:35:03.270180: train_loss -0.8853
2025-11-29 14:35:03.294729: val_loss -0.6271
2025-11-29 14:35:03.298864: Pseudo dice [np.float32(0.7474), np.float32(0.8836)]
2025-11-29 14:35:03.310637: Epoch time: 436.66 s
2025-11-29 14:35:04.209733: 
2025-11-29 14:35:04.213690: Epoch 59
2025-11-29 14:35:04.218309: Current learning rate: 0.00947
2025-11-29 14:42:21.343542: train_loss -0.8852
2025-11-29 14:42:21.348234: val_loss -0.6059
2025-11-29 14:42:21.351178: Pseudo dice [np.float32(0.7228), np.float32(0.8688)]
2025-11-29 14:42:21.353959: Epoch time: 437.13 s
2025-11-29 14:42:22.218672: 
2025-11-29 14:42:22.221699: Epoch 60
2025-11-29 14:42:22.224451: Current learning rate: 0.00946
2025-11-29 14:49:39.306434: train_loss -0.8901
2025-11-29 14:49:39.337018: val_loss -0.5158
2025-11-29 14:49:39.345244: Pseudo dice [np.float32(0.6863), np.float32(0.888)]
2025-11-29 14:49:39.349050: Epoch time: 437.09 s
2025-11-29 14:49:40.375719: 
2025-11-29 14:49:40.380255: Epoch 61
2025-11-29 14:49:40.384044: Current learning rate: 0.00945
2025-11-29 14:56:57.005905: train_loss -0.8823
2025-11-29 14:56:57.013481: val_loss -0.6927
2025-11-29 14:56:57.017145: Pseudo dice [np.float32(0.8405), np.float32(0.8961)]
2025-11-29 14:56:57.019971: Epoch time: 436.63 s
2025-11-29 14:56:57.913846: 
2025-11-29 14:56:57.917068: Epoch 62
2025-11-29 14:56:57.919992: Current learning rate: 0.00944
2025-11-29 15:04:15.172195: train_loss -0.8845
2025-11-29 15:04:15.178355: val_loss -0.575
2025-11-29 15:04:15.181458: Pseudo dice [np.float32(0.6929), np.float32(0.8768)]
2025-11-29 15:04:15.184618: Epoch time: 437.26 s
2025-11-29 15:04:16.240004: 
2025-11-29 15:04:16.247901: Epoch 63
2025-11-29 15:04:16.253249: Current learning rate: 0.00943
2025-11-29 15:11:33.326554: train_loss -0.8902
2025-11-29 15:11:33.331244: val_loss -0.6516
2025-11-29 15:11:33.333701: Pseudo dice [np.float32(0.7711), np.float32(0.8883)]
2025-11-29 15:11:33.336451: Epoch time: 437.09 s
2025-11-29 15:11:34.224743: 
2025-11-29 15:11:34.228930: Epoch 64
2025-11-29 15:11:34.233059: Current learning rate: 0.00942
2025-11-29 15:18:51.334105: train_loss -0.8886
2025-11-29 15:18:51.338072: val_loss -0.7169
2025-11-29 15:18:51.340977: Pseudo dice [np.float32(0.8296), np.float32(0.8965)]
2025-11-29 15:18:51.344025: Epoch time: 437.11 s
2025-11-29 15:18:52.208882: 
2025-11-29 15:18:52.212655: Epoch 65
2025-11-29 15:18:52.226609: Current learning rate: 0.00941
2025-11-29 15:26:09.328986: train_loss -0.8782
2025-11-29 15:26:09.337631: val_loss -0.6396
2025-11-29 15:26:09.344816: Pseudo dice [np.float32(0.7702), np.float32(0.8845)]
2025-11-29 15:26:09.350273: Epoch time: 437.12 s
2025-11-29 15:26:10.438919: 
2025-11-29 15:26:10.466658: Epoch 66
2025-11-29 15:26:10.472238: Current learning rate: 0.0094
2025-11-29 15:33:27.545363: train_loss -0.8695
2025-11-29 15:33:27.550205: val_loss -0.6236
2025-11-29 15:33:27.553401: Pseudo dice [np.float32(0.7007), np.float32(0.8597)]
2025-11-29 15:33:27.559904: Epoch time: 437.11 s
2025-11-29 15:33:28.670256: 
2025-11-29 15:33:28.674143: Epoch 67
2025-11-29 15:33:28.677095: Current learning rate: 0.00939
2025-11-29 15:40:45.205494: train_loss -0.886
2025-11-29 15:40:45.213536: val_loss -0.6439
2025-11-29 15:40:45.216432: Pseudo dice [np.float32(0.7879), np.float32(0.8934)]
2025-11-29 15:40:45.218765: Epoch time: 436.54 s
2025-11-29 15:40:46.090415: 
2025-11-29 15:40:46.112496: Epoch 68
2025-11-29 15:40:46.116523: Current learning rate: 0.00939
2025-11-29 15:48:02.729502: train_loss -0.8778
2025-11-29 15:48:02.735026: val_loss -0.7145
2025-11-29 15:48:02.738310: Pseudo dice [np.float32(0.8041), np.float32(0.897)]
2025-11-29 15:48:02.740613: Epoch time: 436.64 s
2025-11-29 15:48:03.623652: 
2025-11-29 15:48:03.626411: Epoch 69
2025-11-29 15:48:03.629505: Current learning rate: 0.00938
2025-11-29 15:55:20.179854: train_loss -0.8856
2025-11-29 15:55:20.185549: val_loss -0.5909
2025-11-29 15:55:20.188767: Pseudo dice [np.float32(0.7266), np.float32(0.8762)]
2025-11-29 15:55:20.191399: Epoch time: 436.56 s
2025-11-29 15:55:21.431749: 
2025-11-29 15:55:21.436202: Epoch 70
2025-11-29 15:55:21.438908: Current learning rate: 0.00937
2025-11-29 16:02:38.190946: train_loss -0.8677
2025-11-29 16:02:38.198202: val_loss -0.6425
2025-11-29 16:02:38.210960: Pseudo dice [np.float32(0.7454), np.float32(0.8655)]
2025-11-29 16:02:38.214962: Epoch time: 436.76 s
2025-11-29 16:02:39.255795: 
2025-11-29 16:02:39.262193: Epoch 71
2025-11-29 16:02:39.269865: Current learning rate: 0.00936
2025-11-29 16:09:55.726672: train_loss -0.88
2025-11-29 16:09:55.733143: val_loss -0.6401
2025-11-29 16:09:55.735632: Pseudo dice [np.float32(0.7468), np.float32(0.8856)]
2025-11-29 16:09:55.738300: Epoch time: 436.47 s
2025-11-29 16:09:56.609297: 
2025-11-29 16:09:56.612539: Epoch 72
2025-11-29 16:09:56.617066: Current learning rate: 0.00935
2025-11-29 16:17:13.766032: train_loss -0.8776
2025-11-29 16:17:13.775634: val_loss -0.6262
2025-11-29 16:17:13.778805: Pseudo dice [np.float32(0.7387), np.float32(0.8601)]
2025-11-29 16:17:13.781528: Epoch time: 437.16 s
2025-11-29 16:17:14.841409: 
2025-11-29 16:17:14.846954: Epoch 73
2025-11-29 16:17:14.850869: Current learning rate: 0.00934
2025-11-29 16:24:31.846004: train_loss -0.8813
2025-11-29 16:24:31.850132: val_loss -0.6553
2025-11-29 16:24:31.852895: Pseudo dice [np.float32(0.739), np.float32(0.8769)]
2025-11-29 16:24:31.855276: Epoch time: 437.01 s
2025-11-29 16:24:32.733683: 
2025-11-29 16:24:32.736333: Epoch 74
2025-11-29 16:24:32.739123: Current learning rate: 0.00933
2025-11-29 16:31:49.961652: train_loss -0.8898
2025-11-29 16:31:49.969393: val_loss -0.6229
2025-11-29 16:31:49.974127: Pseudo dice [np.float32(0.7245), np.float32(0.8658)]
2025-11-29 16:31:49.977423: Epoch time: 437.23 s
2025-11-29 16:31:51.046039: 
2025-11-29 16:31:51.053427: Epoch 75
2025-11-29 16:31:51.058934: Current learning rate: 0.00932
2025-11-29 16:39:08.078416: train_loss -0.8848
2025-11-29 16:39:08.082972: val_loss -0.7162
2025-11-29 16:39:08.086313: Pseudo dice [np.float32(0.848), np.float32(0.9157)]
2025-11-29 16:39:08.089360: Epoch time: 437.03 s
2025-11-29 16:39:09.170480: 
2025-11-29 16:39:09.176815: Epoch 76
2025-11-29 16:39:09.181585: Current learning rate: 0.00931
2025-11-29 16:46:26.097552: train_loss -0.8844
2025-11-29 16:46:26.113140: val_loss -0.6192
2025-11-29 16:46:26.116427: Pseudo dice [np.float32(0.7363), np.float32(0.8807)]
2025-11-29 16:46:26.119677: Epoch time: 436.93 s
2025-11-29 16:46:27.210503: 
2025-11-29 16:46:27.215331: Epoch 77
2025-11-29 16:46:27.219921: Current learning rate: 0.0093
2025-11-29 16:53:44.284184: train_loss -0.8851
2025-11-29 16:53:44.290046: val_loss -0.7055
2025-11-29 16:53:44.293059: Pseudo dice [np.float32(0.8217), np.float32(0.9059)]
2025-11-29 16:53:44.295619: Epoch time: 437.07 s
2025-11-29 16:53:45.269920: 
2025-11-29 16:53:45.276719: Epoch 78
2025-11-29 16:53:45.279657: Current learning rate: 0.0093
2025-11-29 17:01:02.087087: train_loss -0.8934
2025-11-29 17:01:02.092139: val_loss -0.6442
2025-11-29 17:01:02.094797: Pseudo dice [np.float32(0.691), np.float32(0.8525)]
2025-11-29 17:01:02.097677: Epoch time: 436.82 s
2025-11-29 17:01:02.992820: 
2025-11-29 17:01:02.996943: Epoch 79
2025-11-29 17:01:03.009572: Current learning rate: 0.00929
2025-11-29 17:08:19.822321: train_loss -0.8902
2025-11-29 17:08:19.828273: val_loss -0.7809
2025-11-29 17:08:19.831267: Pseudo dice [np.float32(0.897), np.float32(0.9162)]
2025-11-29 17:08:19.834035: Epoch time: 436.83 s
2025-11-29 17:08:19.836751: Yayy! New best EMA pseudo Dice: 0.8284000158309937
2025-11-29 17:08:21.920143: 
2025-11-29 17:08:21.925364: Epoch 80
2025-11-29 17:08:21.928985: Current learning rate: 0.00928
2025-11-29 17:15:39.323812: train_loss -0.8929
2025-11-29 17:15:39.329259: val_loss -0.695
2025-11-29 17:15:39.332149: Pseudo dice [np.float32(0.8317), np.float32(0.9003)]
2025-11-29 17:15:39.338826: Epoch time: 437.4 s
2025-11-29 17:15:39.342590: Yayy! New best EMA pseudo Dice: 0.8321999907493591
2025-11-29 17:15:41.454574: 
2025-11-29 17:15:41.458989: Epoch 81
2025-11-29 17:15:41.462754: Current learning rate: 0.00927
2025-11-29 17:22:58.306432: train_loss -0.8873
2025-11-29 17:22:58.316517: val_loss -0.6554
2025-11-29 17:22:58.319682: Pseudo dice [np.float32(0.7898), np.float32(0.9001)]
2025-11-29 17:22:58.322486: Epoch time: 436.85 s
2025-11-29 17:22:58.326370: Yayy! New best EMA pseudo Dice: 0.8335000276565552
2025-11-29 17:23:00.095157: 
2025-11-29 17:23:00.098822: Epoch 82
2025-11-29 17:23:00.109030: Current learning rate: 0.00926
2025-11-29 17:30:16.774475: train_loss -0.8876
2025-11-29 17:30:16.782462: val_loss -0.5241
2025-11-29 17:30:16.785048: Pseudo dice [np.float32(0.6456), np.float32(0.86)]
2025-11-29 17:30:16.787872: Epoch time: 436.68 s
2025-11-29 17:30:17.751280: 
2025-11-29 17:30:17.754144: Epoch 83
2025-11-29 17:30:17.756672: Current learning rate: 0.00925
2025-11-29 17:37:34.124893: train_loss -0.8916
2025-11-29 17:37:34.129749: val_loss -0.6192
2025-11-29 17:37:34.132235: Pseudo dice [np.float32(0.7339), np.float32(0.8885)]
2025-11-29 17:37:34.134444: Epoch time: 436.37 s
2025-11-29 17:37:34.994311: 
2025-11-29 17:37:35.009161: Epoch 84
2025-11-29 17:37:35.012995: Current learning rate: 0.00924
2025-11-29 17:44:51.984183: train_loss -0.8939
2025-11-29 17:44:51.990054: val_loss -0.6004
2025-11-29 17:44:51.992745: Pseudo dice [np.float32(0.7248), np.float32(0.8837)]
2025-11-29 17:44:51.995222: Epoch time: 436.99 s
2025-11-29 17:44:52.864628: 
2025-11-29 17:44:52.881570: Epoch 85
2025-11-29 17:44:52.884885: Current learning rate: 0.00923
2025-11-29 17:52:10.033566: train_loss -0.8938
2025-11-29 17:52:10.042072: val_loss -0.709
2025-11-29 17:52:10.046635: Pseudo dice [np.float32(0.8207), np.float32(0.8972)]
2025-11-29 17:52:10.050975: Epoch time: 437.17 s
2025-11-29 17:52:11.231649: 
2025-11-29 17:52:11.242614: Epoch 86
2025-11-29 17:52:11.250667: Current learning rate: 0.00922
2025-11-29 17:59:28.946398: train_loss -0.8977
2025-11-29 17:59:28.951773: val_loss -0.6548
2025-11-29 17:59:28.955154: Pseudo dice [np.float32(0.7595), np.float32(0.8991)]
2025-11-29 17:59:28.958873: Epoch time: 437.72 s
2025-11-29 17:59:29.999629: 
2025-11-29 17:59:30.011444: Epoch 87
2025-11-29 17:59:30.016115: Current learning rate: 0.00921
2025-11-29 18:06:46.977706: train_loss -0.8934
2025-11-29 18:06:46.983143: val_loss -0.6874
2025-11-29 18:06:46.986250: Pseudo dice [np.float32(0.8308), np.float32(0.9062)]
2025-11-29 18:06:46.988356: Epoch time: 436.98 s
2025-11-29 18:06:48.012607: 
2025-11-29 18:06:48.017654: Epoch 88
2025-11-29 18:06:48.020082: Current learning rate: 0.0092
2025-11-29 18:14:04.886261: train_loss -0.8927
2025-11-29 18:14:04.891938: val_loss -0.6742
2025-11-29 18:14:04.895025: Pseudo dice [np.float32(0.7664), np.float32(0.8848)]
2025-11-29 18:14:04.899121: Epoch time: 436.87 s
2025-11-29 18:14:05.938227: 
2025-11-29 18:14:05.943495: Epoch 89
2025-11-29 18:14:05.946539: Current learning rate: 0.0092
2025-11-29 18:21:22.859123: train_loss -0.8914
2025-11-29 18:21:22.864509: val_loss -0.66
2025-11-29 18:21:22.866611: Pseudo dice [np.float32(0.766), np.float32(0.8842)]
2025-11-29 18:21:22.869030: Epoch time: 436.92 s
2025-11-29 18:21:23.734558: 
2025-11-29 18:21:23.738344: Epoch 90
2025-11-29 18:21:23.741221: Current learning rate: 0.00919
2025-11-29 18:28:40.254411: train_loss -0.8961
2025-11-29 18:28:40.259003: val_loss -0.6149
2025-11-29 18:28:40.261343: Pseudo dice [np.float32(0.7075), np.float32(0.8738)]
2025-11-29 18:28:40.272219: Epoch time: 436.52 s
2025-11-29 18:28:41.410026: 
2025-11-29 18:28:41.413784: Epoch 91
2025-11-29 18:28:41.416486: Current learning rate: 0.00918
2025-11-29 18:35:58.428677: train_loss -0.8937
2025-11-29 18:35:58.435338: val_loss -0.6703
2025-11-29 18:35:58.438149: Pseudo dice [np.float32(0.7722), np.float32(0.8943)]
2025-11-29 18:35:58.440319: Epoch time: 437.02 s
2025-11-29 18:35:59.334259: 
2025-11-29 18:35:59.337452: Epoch 92
2025-11-29 18:35:59.341143: Current learning rate: 0.00917
2025-11-29 18:43:16.012611: train_loss -0.895
2025-11-29 18:43:16.019128: val_loss -0.6441
2025-11-29 18:43:16.022217: Pseudo dice [np.float32(0.7822), np.float32(0.9001)]
2025-11-29 18:43:16.025411: Epoch time: 436.68 s
2025-11-29 18:43:17.040883: 
2025-11-29 18:43:17.047439: Epoch 93
2025-11-29 18:43:17.051950: Current learning rate: 0.00916
2025-11-29 18:50:33.999692: train_loss -0.8891
2025-11-29 18:50:34.015172: val_loss -0.5739
2025-11-29 18:50:34.019435: Pseudo dice [np.float32(0.7034), np.float32(0.8672)]
2025-11-29 18:50:34.022856: Epoch time: 436.96 s
2025-11-29 18:50:35.310705: 
2025-11-29 18:50:35.315939: Epoch 94
2025-11-29 18:50:35.318736: Current learning rate: 0.00915
2025-11-29 18:57:52.204390: train_loss -0.8834
2025-11-29 18:57:52.213080: val_loss -0.7313
2025-11-29 18:57:52.215719: Pseudo dice [np.float32(0.8381), np.float32(0.8941)]
2025-11-29 18:57:52.218141: Epoch time: 436.89 s
2025-11-29 18:57:53.070787: 
2025-11-29 18:57:53.074135: Epoch 95
2025-11-29 18:57:53.077688: Current learning rate: 0.00914
2025-11-29 19:05:10.144131: train_loss -0.888
2025-11-29 19:05:10.148834: val_loss -0.6259
2025-11-29 19:05:10.151503: Pseudo dice [np.float32(0.7251), np.float32(0.8823)]
2025-11-29 19:05:10.153543: Epoch time: 437.07 s
2025-11-29 19:05:11.116889: 
2025-11-29 19:05:11.119272: Epoch 96
2025-11-29 19:05:11.121760: Current learning rate: 0.00913
2025-11-29 19:12:28.147455: train_loss -0.8971
2025-11-29 19:12:28.152512: val_loss -0.6517
2025-11-29 19:12:28.155120: Pseudo dice [np.float32(0.7536), np.float32(0.8902)]
2025-11-29 19:12:28.158028: Epoch time: 437.03 s
2025-11-29 19:12:29.027680: 
2025-11-29 19:12:29.030261: Epoch 97
2025-11-29 19:12:29.033311: Current learning rate: 0.00912
2025-11-29 19:19:45.655180: train_loss -0.9021
2025-11-29 19:19:45.660633: val_loss -0.6768
2025-11-29 19:19:45.663355: Pseudo dice [np.float32(0.7897), np.float32(0.8996)]
2025-11-29 19:19:45.665967: Epoch time: 436.63 s
2025-11-29 19:19:46.714808: 
2025-11-29 19:19:46.718859: Epoch 98
2025-11-29 19:19:46.722295: Current learning rate: 0.00911
2025-11-29 19:27:03.545832: train_loss -0.908
2025-11-29 19:27:03.552402: val_loss -0.6714
2025-11-29 19:27:03.555646: Pseudo dice [np.float32(0.7769), np.float32(0.8932)]
2025-11-29 19:27:03.558406: Epoch time: 436.83 s
2025-11-29 19:27:04.428122: 
2025-11-29 19:27:04.431308: Epoch 99
2025-11-29 19:27:04.434376: Current learning rate: 0.0091
2025-11-29 19:34:20.030459: train_loss -0.9032
2025-11-29 19:34:20.035704: val_loss -0.6772
2025-11-29 19:34:20.038268: Pseudo dice [np.float32(0.7638), np.float32(0.8819)]
2025-11-29 19:34:20.040963: Epoch time: 435.6 s
2025-11-29 19:34:21.838275: 
2025-11-29 19:34:21.844203: Epoch 100
2025-11-29 19:34:21.847423: Current learning rate: 0.0091
2025-11-29 19:41:38.629152: train_loss -0.9018
2025-11-29 19:41:38.634160: val_loss -0.6564
2025-11-29 19:41:38.637348: Pseudo dice [np.float32(0.746), np.float32(0.8729)]
2025-11-29 19:41:38.640046: Epoch time: 436.79 s
2025-11-29 19:41:39.514526: 
2025-11-29 19:41:39.517714: Epoch 101
2025-11-29 19:41:39.520847: Current learning rate: 0.00909
2025-11-29 19:48:56.687300: train_loss -0.9089
2025-11-29 19:48:56.694607: val_loss -0.655
2025-11-29 19:48:56.697807: Pseudo dice [np.float32(0.7646), np.float32(0.8871)]
2025-11-29 19:48:56.708144: Epoch time: 437.17 s
2025-11-29 19:48:57.777178: 
2025-11-29 19:48:57.782250: Epoch 102
2025-11-29 19:48:57.785077: Current learning rate: 0.00908
2025-11-29 19:56:14.577717: train_loss -0.9088
2025-11-29 19:56:14.587616: val_loss -0.7265
2025-11-29 19:56:14.590736: Pseudo dice [np.float32(0.8195), np.float32(0.8975)]
2025-11-29 19:56:14.593570: Epoch time: 436.8 s
2025-11-29 19:56:15.458380: 
2025-11-29 19:56:15.461920: Epoch 103
2025-11-29 19:56:15.465756: Current learning rate: 0.00907
2025-11-29 20:03:32.078085: train_loss -0.9021
2025-11-29 20:03:32.087280: val_loss -0.7226
2025-11-29 20:03:32.090110: Pseudo dice [np.float32(0.8347), np.float32(0.9034)]
2025-11-29 20:03:32.093146: Epoch time: 436.62 s
2025-11-29 20:03:33.123380: 
2025-11-29 20:03:33.132690: Epoch 104
2025-11-29 20:03:33.140226: Current learning rate: 0.00906
